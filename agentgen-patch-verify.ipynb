{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaL4","dataSources":[{"sourceId":84795,"databundleVersionId":11281725,"sourceType":"competition"},{"sourceId":221096520,"sourceType":"kernelVersion"},{"sourceId":236932,"sourceType":"modelInstanceVersion","modelInstanceId":202348,"modelId":224071},{"sourceId":237029,"sourceType":"modelInstanceVersion","modelInstanceId":202436,"modelId":224071},{"sourceId":256551,"sourceType":"modelInstanceVersion","modelInstanceId":205979,"modelId":225262},{"sourceId":256580,"sourceType":"modelInstanceVersion","modelInstanceId":204048,"modelId":225262},{"sourceId":265863,"sourceType":"modelInstanceVersion","modelInstanceId":227466,"modelId":224053}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n\n# https://www.kaggle.com/competitions/ai-mathematical-olympiad-progress-prize-2/discussion/560682#3113134\nos.environ[\"TRITON_PTXAS_PATH\"] = \"/usr/local/cuda/bin/ptxas\"","metadata":{"_uuid":"38b86f00-408f-4577-b91e-890da62fd434","_cell_guid":"eec2b282-90f3-4965-a943-54cffbeb7a5b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-07T23:39:03.331652Z","iopub.execute_input":"2025-03-07T23:39:03.331843Z","iopub.status.idle":"2025-03-07T23:39:03.334699Z","shell.execute_reply.started":"2025-03-07T23:39:03.331824Z","shell.execute_reply":"2025-03-07T23:39:03.334143Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import io\nimport time\nimport shutil\n\nimport pandas as pd\nimport polars as pl\n\nimport kaggle_evaluation.konwinski_prize_inference_server\nfrom typing import List, Tuple, Dict, Optional\n\nstart_time = time.time()","metadata":{"_uuid":"e9ab5832-846d-4fb5-b21b-532f5962301d","_cell_guid":"2512bd4d-9caf-4c50-9706-0377f47811ae","trusted":true,"collapsed":false,"_kg_hide-output":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-07T23:39:03.335466Z","iopub.execute_input":"2025-03-07T23:39:03.335655Z","iopub.status.idle":"2025-03-07T23:39:21.527779Z","shell.execute_reply.started":"2025-03-07T23:39:03.335638Z","shell.execute_reply":"2025-03-07T23:39:21.527091Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"The evaluation API requires that you set up a server which will respond to inference requests. We have already defined the server; you just need write the predict function. When we evaluate your submission on the hidden test set the client defined in `konwinski_prize_gateway` will run in a different container with direct access to the hidden test set and hand off the data.\n#\nYour code will always have access to the published copies of the files.","metadata":{"_uuid":"acfccb6c-54ff-466d-b3ca-dc9d610e1562","_cell_guid":"9b41a507-839f-49ba-bed3-7ec21f5bd402","trusted":true,"collapsed":false,"papermill":{"duration":0.002032,"end_time":"2024-12-11T03:22:08.823897","exception":false,"start_time":"2024-12-11T03:22:08.821865","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"instance_count: Optional[int] = None\n\n\ndef get_number_of_instances(num_instances: int) -> None:\n    \"\"\"The very first message from the gateway will be the total number of instances to be served.\n    You don't need to edit this function.\n    \"\"\"\n    global instance_count\n    instance_count = num_instances","metadata":{"_uuid":"f251dce7-62ae-4856-9c05-f5ffe2938eaf","_cell_guid":"7fce3650-d66d-4189-ac61-51f4e07ac289","trusted":true,"collapsed":false,"papermill":{"duration":0.011949,"end_time":"2024-12-11T03:22:08.838279","exception":false,"start_time":"2024-12-11T03:22:08.82633","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-07T23:39:21.529155Z","iopub.execute_input":"2025-03-07T23:39:21.529766Z","iopub.status.idle":"2025-03-07T23:39:21.533131Z","shell.execute_reply.started":"2025-03-07T23:39:21.529742Z","shell.execute_reply":"2025-03-07T23:39:21.532487Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Initialize LLM","metadata":{"_uuid":"370e67e3-a871-42b0-b0a3-ab7bbcf87dcc","_cell_guid":"e00a129e-16b7-408e-a131-75c4c476b300","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"from vllm import LLM, SamplingParams, RequestOutput\nimport warnings\n\nwarnings.simplefilter(\"ignore\")\n\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nif os.getenv(\"KAGGLE_KERNEL_RUN_TYPE\") or os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n    llm_model_pth: str = (\n        \"/kaggle/input/m/shelterw/deepseek-r1/transformers/deepseek-r1-distill-qwen-32b-awq/1\"\n    )\nelse:\n    llm_model_pth: str = \"/root/volume/KirillR/QwQ-32B-Preview-AWQ\"\n\nBATCH_SIZE: int = 6\nVALIDATION_COPY_COUNT: int = 1\nMAX_TOKENS: int = 4096\n\nMAX_NUM_SEQS: int = 6\nMAX_MODEL_LEN: int = 32_768\n\nllm: LLM = LLM(\n    llm_model_pth,\n    max_num_seqs=MAX_NUM_SEQS,  # Maximum number of sequences per iteration. Default is 256\n    max_model_len=MAX_MODEL_LEN,  # Model context length\n    trust_remote_code=True,  # Trust remote code (e.g., from HuggingFace) when downloading the model and tokenizer\n    tensor_parallel_size=4,  # The number of GPUs to use for distributed execution with tensor parallelism\n    gpu_memory_utilization=0.95,  # The ratio (between 0 and 1) of GPU memory to reserve for the model\n    seed=2024,\n)","metadata":{"_uuid":"eca28397-2a40-4265-b36e-dfa9d80eb2b1","_cell_guid":"c863b090-e04a-441e-9add-bddae0863ef9","trusted":true,"collapsed":false,"_kg_hide-output":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-07T23:39:21.535653Z","iopub.execute_input":"2025-03-07T23:39:21.535849Z","iopub.status.idle":"2025-03-07T23:44:07.827092Z","shell.execute_reply.started":"2025-03-07T23:39:21.535832Z","shell.execute_reply":"2025-03-07T23:44:07.826249Z"}},"outputs":[{"name":"stdout","text":"INFO 03-07 23:40:16 __init__.py:183] Automatically detected platform cuda.\nINFO 03-07 23:40:52 config.py:526] This model supports multiple tasks: {'score', 'generate', 'classify', 'embed', 'reward'}. Defaulting to 'generate'.\nINFO 03-07 23:40:55 awq_marlin.py:109] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\nINFO 03-07 23:40:56 config.py:1383] Defaulting to use mp for distributed inference\nWARNING 03-07 23:40:56 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\nINFO 03-07 23:40:56 llm_engine.py:232] Initializing a V0 LLM engine (v0.7.1) with config: model='/kaggle/input/m/shelterw/deepseek-r1/transformers/deepseek-r1-distill-qwen-32b-awq/1', speculative_config=None, tokenizer='/kaggle/input/m/shelterw/deepseek-r1/transformers/deepseek-r1-distill-qwen-32b-awq/1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=2024, served_model_name=/kaggle/input/m/shelterw/deepseek-r1/transformers/deepseek-r1-distill-qwen-32b-awq/1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[8,4,2,1],\"max_capture_size\":8}, use_cached_outputs=False, \nWARNING 03-07 23:40:56 multiproc_worker_utils.py:298] Reducing Torch parallelism from 24 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\nINFO 03-07 23:40:56 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n\u001b[1;36m(VllmWorkerProcess pid=534)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=533)\u001b[0;0m INFO 03-07 23:40:56 multiproc_worker_utils.py:227] Worker ready; awaiting tasks\nINFO 03-07 23:40:56 multiproc_worker_utils.py:227] Worker ready; awaiting tasks\n\u001b[1;36m(VllmWorkerProcess pid=539)\u001b[0;0m INFO 03-07 23:40:56 multiproc_worker_utils.py:227] Worker ready; awaiting tasks\nWARNING 03-07 23:40:57 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n\u001b[1;36m(VllmWorkerProcess pid=534)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=539)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=533)\u001b[0;0m WARNING 03-07 23:40:57 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\nWARNING 03-07 23:40:57 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\nWARNING 03-07 23:40:57 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n\u001b[1;36m(VllmWorkerProcess pid=533)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=539)\u001b[0;0m INFO 03-07 23:40:57 cuda.py:235] Using Flash Attention backend.\n\u001b[1;36m(VllmWorkerProcess pid=534)\u001b[0;0m INFO 03-07 23:40:57 cuda.py:235] Using Flash Attention backend.\nINFO 03-07 23:40:57 cuda.py:235] Using Flash Attention backend.\nINFO 03-07 23:40:57 cuda.py:235] Using Flash Attention backend.\nINFO 03-07 23:41:08 utils.py:938] Found nccl from library libnccl.so.2\nINFO 03-07 23:41:08 pynccl.py:67] vLLM is using nccl==2.21.5\n\u001b[1;36m(VllmWorkerProcess pid=533)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=534)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=539)\u001b[0;0m INFO 03-07 23:41:08 utils.py:938] Found nccl from library libnccl.so.2\nINFO 03-07 23:41:08 utils.py:938] Found nccl from library libnccl.so.2\nINFO 03-07 23:41:08 utils.py:938] Found nccl from library libnccl.so.2\n\u001b[1;36m(VllmWorkerProcess pid=533)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=534)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=539)\u001b[0;0m INFO 03-07 23:41:08 pynccl.py:67] vLLM is using nccl==2.21.5\nINFO 03-07 23:41:08 pynccl.py:67] vLLM is using nccl==2.21.5\nINFO 03-07 23:41:08 pynccl.py:67] vLLM is using nccl==2.21.5\nWARNING 03-07 23:41:09 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n\u001b[1;36m(VllmWorkerProcess pid=539)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=534)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=533)\u001b[0;0m WARNING 03-07 23:41:09 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\nWARNING 03-07 23:41:09 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\nWARNING 03-07 23:41:09 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\nINFO 03-07 23:41:09 shm_broadcast.py:256] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_2f670fcf'), local_subscribe_port=59167, remote_subscribe_port=None)\nINFO 03-07 23:41:09 model_runner.py:1111] Starting to load model /kaggle/input/m/shelterw/deepseek-r1/transformers/deepseek-r1-distill-qwen-32b-awq/1...\n\u001b[1;36m(VllmWorkerProcess pid=533)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=539)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=534)\u001b[0;0m INFO 03-07 23:41:09 model_runner.py:1111] Starting to load model /kaggle/input/m/shelterw/deepseek-r1/transformers/deepseek-r1-distill-qwen-32b-awq/1...\nINFO 03-07 23:41:09 model_runner.py:1111] Starting to load model /kaggle/input/m/shelterw/deepseek-r1/transformers/deepseek-r1-distill-qwen-32b-awq/1...\nINFO 03-07 23:41:09 model_runner.py:1111] Starting to load model /kaggle/input/m/shelterw/deepseek-r1/transformers/deepseek-r1-distill-qwen-32b-awq/1...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dabb8c1c04ab46eb82aab6896d76880a"}},"metadata":{}},{"name":"stdout","text":"\u001b[1;36m(VllmWorkerProcess pid=533)\u001b[0;0m INFO 03-07 23:43:18 model_runner.py:1116] Loading model weights took 4.5673 GB\nINFO 03-07 23:43:18 model_runner.py:1116] Loading model weights took 4.5673 GB\n\u001b[1;36m(VllmWorkerProcess pid=539)\u001b[0;0m INFO 03-07 23:43:18 model_runner.py:1116] Loading model weights took 4.5673 GB\n\u001b[1;36m(VllmWorkerProcess pid=534)\u001b[0;0m INFO 03-07 23:43:18 model_runner.py:1116] Loading model weights took 4.5673 GB\n\u001b[1;36m(VllmWorkerProcess pid=534)\u001b[0;0m WARNING 03-07 23:43:59 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n\u001b[1;36m(VllmWorkerProcess pid=533)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=534)\u001b[0;0m WARNING 03-07 23:43:59 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\nWARNING 03-07 23:43:59 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n\u001b[1;36m(VllmWorkerProcess pid=533)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=534)\u001b[0;0m WARNING 03-07 23:43:59 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\nINFO 03-07 23:43:59 worker.py:266] Memory profiling takes 39.70 seconds\n\u001b[1;36m(VllmWorkerProcess pid=533)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=539)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=534)\u001b[0;0m INFO 03-07 23:43:59 worker.py:266] Memory profiling takes 39.71 seconds\nWARNING 03-07 23:43:59 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\nINFO 03-07 23:43:59 worker.py:266] the current vLLM instance can use total_gpu_memory (22.28GiB) x gpu_memory_utilization (0.95) = 21.16GiB\n\u001b[1;36m(VllmWorkerProcess pid=533)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=534)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=539)\u001b[0;0m INFO 03-07 23:43:59 worker.py:266] the current vLLM instance can use total_gpu_memory (22.28GiB) x gpu_memory_utilization (0.95) = 21.16GiB\nINFO 03-07 23:43:59 worker.py:266] model weights take 4.57GiB; non_torch_memory takes 0.17GiB; PyTorch activation peak memory takes 2.96GiB; the rest of the memory reserved for KV Cache is 13.47GiB.\nWARNING 03-07 23:43:59 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n\u001b[1;36m(VllmWorkerProcess pid=533)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=539)\u001b[0;0m INFO 03-07 23:43:59 worker.py:266] model weights take 4.57GiB; non_torch_memory takes 0.17GiB; PyTorch activation peak memory takes 2.96GiB; the rest of the memory reserved for KV Cache is 13.47GiB.\nINFO 03-07 23:43:59 worker.py:266] Memory profiling takes 39.71 seconds\n\u001b[1;36m(VllmWorkerProcess pid=539)\u001b[0;0m INFO 03-07 23:43:59 worker.py:266] the current vLLM instance can use total_gpu_memory (22.28GiB) x gpu_memory_utilization (0.95) = 21.16GiB\n\u001b[1;36m(VllmWorkerProcess pid=539)\u001b[0;0m INFO 03-07 23:43:59 worker.py:266] model weights take 4.57GiB; non_torch_memory takes 0.17GiB; PyTorch activation peak memory takes 2.96GiB; the rest of the memory reserved for KV Cache is 13.47GiB.\nWARNING 03-07 23:43:59 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\nWARNING 03-07 23:43:59 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\nINFO 03-07 23:43:59 worker.py:266] Memory profiling takes 40.03 seconds\nINFO 03-07 23:43:59 worker.py:266] the current vLLM instance can use total_gpu_memory (22.28GiB) x gpu_memory_utilization (0.95) = 21.16GiB\nINFO 03-07 23:43:59 worker.py:266] model weights take 4.57GiB; non_torch_memory takes 0.17GiB; PyTorch activation peak memory takes 2.96GiB; the rest of the memory reserved for KV Cache is 13.47GiB.\nINFO 03-07 23:43:59 executor_base.py:108] # CUDA blocks: 13794, # CPU blocks: 4096\nINFO 03-07 23:43:59 executor_base.py:113] Maximum concurrency for 32768 tokens per request: 6.74x\nWARNING 03-07 23:43:59 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\nWARNING 03-07 23:43:59 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n\u001b[1;36m(VllmWorkerProcess pid=539)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=534)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=533)\u001b[0;0m WARNING 03-07 23:43:59 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\nWARNING 03-07 23:43:59 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n\u001b[1;36m(VllmWorkerProcess pid=534)\u001b[0;0m WARNING 03-07 23:43:59 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n\u001b[1;36m(VllmWorkerProcess pid=539)\u001b[0;0m WARNING 03-07 23:43:59 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\nWARNING 03-07 23:43:59 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n\u001b[1;36m(VllmWorkerProcess pid=533)\u001b[0;0m WARNING 03-07 23:43:59 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n\u001b[1;36m(VllmWorkerProcess pid=534)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=533)\u001b[0;0m INFO 03-07 23:44:02 model_runner.py:1435] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\nINFO 03-07 23:44:02 model_runner.py:1435] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\nINFO 03-07 23:44:02 model_runner.py:1435] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n\u001b[1;36m(VllmWorkerProcess pid=539)\u001b[0;0m INFO 03-07 23:44:02 model_runner.py:1435] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n","output_type":"stream"},{"name":"stderr","text":"Capturing CUDA graph shapes: 100%|██████████| 4/4 [00:04<00:00,  1.26s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1;36m(VllmWorkerProcess pid=533)\u001b[0;0m ","output_type":"stream"},{"name":"stderr","text":"Capturing CUDA graph shapes: 100%|██████████| 4/4 [00:04<00:00,  1.21s/it]","output_type":"stream"},{"name":"stdout","text":"\u001b[1;36m(VllmWorkerProcess pid=539)\u001b[0;0m INFO 03-07 23:44:07 model_runner.py:1563] Graph capturing finished in 5 secs, took 0.08 GiB\nINFO 03-07 23:44:07 model_runner.py:1563] Graph capturing finished in 5 secs, took 0.08 GiB\nINFO 03-07 23:44:07 model_runner.py:1563] Graph capturing finished in 5 secs, took 0.08 GiB\n\u001b[1;36m(VllmWorkerProcess pid=534)\u001b[0;0m INFO 03-07 23:44:07 model_runner.py:1563] Graph capturing finished in 5 secs, took 0.08 GiB\nINFO 03-07 23:44:07 llm_engine.py:429] init engine (profile, create kv cache, warmup model) took 48.61 seconds\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"tokenizer = llm.get_tokenizer()\n\n\ndef count_tokens(text: str) -> int:\n    return len(tokenizer.encode(text))","metadata":{"_uuid":"5a982fe4-e4e2-466d-b5bf-4839ec8804c8","_cell_guid":"99e40ae3-6474-4b05-a969-0b48de303ba6","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-07T23:44:07.828038Z","iopub.execute_input":"2025-03-07T23:44:07.828306Z","iopub.status.idle":"2025-03-07T23:44:07.831644Z","shell.execute_reply.started":"2025-03-07T23:44:07.828283Z","shell.execute_reply":"2025-03-07T23:44:07.831034Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# Helper functions","metadata":{"_uuid":"30f587b9-7b4f-4812-82f7-d87787c58d1c","_cell_guid":"3a54e069-fe31-4873-9d25-a02c38576c59","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import os\n\n\ndef stringify_directory(directory: str) -> str:\n    full_paths: List[str] = []\n\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            full_path: str = os.path.join(root, file)\n            full_paths.append(full_path)\n    return \"\\n\".join(full_paths)","metadata":{"_uuid":"43546fee-8473-4f0d-80ec-46a86369426b","_cell_guid":"44d40acd-8428-4813-ba5d-62f625309dd4","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-07T23:44:07.832339Z","iopub.execute_input":"2025-03-07T23:44:07.832543Z","iopub.status.idle":"2025-03-07T23:44:07.846225Z","shell.execute_reply.started":"2025-03-07T23:44:07.832527Z","shell.execute_reply":"2025-03-07T23:44:07.845625Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import re\n\n\ndef extract_file_query(xml_content: str) -> Dict[str, List[str]]:\n    import xml.etree.ElementTree as ET\n\n    # Prepare a data structure to collect results\n    parsed_data: Dict[str, List[str]] = {}\n    pattern: str = r\"<root>(.*?)</root>\"\n    matches: List[str] = re.findall(pattern, xml_content, re.DOTALL)\n\n    for match in matches:\n        try:\n            # Parse the XML\n            root = ET.fromstring(\"<root>\" + match + \"</root>\")\n\n            # Find all <entry> elements\n            for entry in root.findall(\"entry\"):\n                # Extract the <filepath> text\n                filepath = entry.find(\"filepath\")\n                filepath_text: Optional[str] = (\n                    filepath.text.strip()\n                    if filepath is not None and filepath.text is not None\n                    else None\n                )\n\n                # Locate <strings_to_search> container\n                strings_container = entry.find(\"strings_to_search\")\n\n                # Gather each <string_to_search> text\n                search_strings: List[str] = []\n                if strings_container is not None:\n                    for s in strings_container.findall(\"string_to_search\"):\n                        if s.text is not None:\n                            search_strings.append(s.text.strip())\n\n                # Store in a dictionary: { filepath: [search_strings...] }\n                parsed_data[filepath_text] = search_strings  # type: ignore\n        except:\n            print(\"Error parsing output\")\n            print(xml_content)\n            return {}\n\n    return parsed_data","metadata":{"_uuid":"47b13790-f9e8-47cd-8b78-c93567e04b56","_cell_guid":"50b46124-4395-4783-b523-f11f4b6228bf","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-07T23:44:07.846860Z","iopub.execute_input":"2025-03-07T23:44:07.847078Z","iopub.status.idle":"2025-03-07T23:44:07.859925Z","shell.execute_reply.started":"2025-03-07T23:44:07.847061Z","shell.execute_reply":"2025-03-07T23:44:07.859325Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"reading_prompt: str = (\n    \"\"\"\nYou will be implementing a git diff patch to solve an issue with the code repository.\nYou will first need to select files in the file directory.\n\nThis is the problem statement.\n\n{problem_statement}\n\nThis is the file directory\n\n<directory>\n{directory_string}\n</directory>\n\nWhich files should be inspected so that we can solve the problem?\nWhen we inspect each file, what strings should be searched?\n\nReturn the strings to search in this format\n\n(explanation)\n\n<root>\n    <entry>\n        <filepath>filepath</filepath>  \n        <strings_to_search>\n            <string_to_search>string_to_search</string_to_search>\n            ...\n            <string_to_search>string_to_search</string_to_search>\n        </strings_to_search>\n    </entry>\n    <entry>\n        <filepath>filepath</filepath>\n        <strings_to_search>\n            <string_to_search>string_to_search</string_to_search>\n            ...\n            <string_to_search>string_to_search</string_to_search>\n        </strings_to_search>\n    </entry>\n    ...\n</root>\n...\n\nNotes:\n- Make sure to encode each entry between <root> and </root>\n- Return the FULL filepath - exactly as specified in <directory> and </directory>\n    - Example: <filepath>repo/path/to/directory/file.py</filepath>\n- If you are searching for a word instead of a substring, maybe add spaces or brackets before and after the string\n    - For example, if you are searching for uses of the function `calculate`, use ` calculate(` as the search string instead of `calculate`\n- Prefer searching longer strings\n    - Avoid searching for strings that might appear in many parts of the codebase\n- Search the test files as well to understand the feature behavior\n    - Also search for the relevant function calls in the test files\n\"\"\".strip()\n)\n\n\ndef get_selection_query(\n    directory_string: str, problem_statement: str\n) -> Tuple[List[str], List[Dict[str, List[str]]]]:\n    sampling_params: SamplingParams = SamplingParams(\n        temperature=0.6,  # randomness of the sampling\n        min_p=0.01,\n        skip_special_tokens=True,  # Whether to skip special tokens in the output\n        max_tokens=MAX_TOKENS,\n    )\n\n    list_of_messages: List[List[Dict[str, str]]] = [\n        [\n            {\n                \"role\": \"user\",\n                \"content\": reading_prompt.format(\n                    problem_statement=problem_statement[:20_000],\n                    directory_string=directory_string[:30_000],\n                ),\n            },\n        ]\n        for _ in range(BATCH_SIZE)\n    ]\n\n    prompt_texts: List[str] = [\n        (\n            tokenizer.apply_chat_template(\n                conversation=messages, tokenize=False, add_generation_prompt=True\n            )  # type: ignore\n        )\n        + \"<think>\\n\"\n        for messages in list_of_messages\n    ]\n    # print(prompt_texts)\n\n    print(\"get_selection_query\", [count_tokens(text) for text in prompt_texts])\n    request_outputs: list[RequestOutput] = llm.generate(\n        prompt_texts, sampling_params=sampling_params\n    )\n    if not request_outputs:\n        return [], []\n    response_texts: List[str] = [\n        request_output.outputs[0].text for request_output in request_outputs\n    ]\n    print(\"get_selection_query\", [count_tokens(text) for text in response_texts])\n\n    completion_texts = [\n        prompt_text + response_text\n        for prompt_text, response_text in zip(prompt_texts, response_texts)\n    ]\n    file_queries: List[Dict[str, List[str]]] = [\n        extract_file_query(response_text) for response_text in response_texts\n    ]\n    return completion_texts, file_queries","metadata":{"_uuid":"b6264935-32cf-49b5-b1da-aece43dbf184","_cell_guid":"a636bf5a-ec3a-4113-8504-bc95550036ef","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-07T23:44:07.862064Z","iopub.execute_input":"2025-03-07T23:44:07.862270Z","iopub.status.idle":"2025-03-07T23:44:07.870933Z","shell.execute_reply.started":"2025-03-07T23:44:07.862253Z","shell.execute_reply":"2025-03-07T23:44:07.870336Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"REPO_PATH: str = \"repo\"\n\n\ndef fetch_file_contents(\n    files_to_search: Dict[str, List[str]], context_lines: int = 12, max_gap: int = 0\n) -> str:\n    from io import StringIO\n    from typing import Tuple\n\n    def find_lines_in_files_with_context(\n        search_map: Dict[str, List[str]], context_lines: int = context_lines\n    ) -> List[List[List[Tuple[int, str]]]]:\n        \"\"\"\n        Given a dictionary mapping file paths to a list of search terms,\n        open each file and gather *snippets* of lines that contain any\n        of those search terms, including 'context_lines' before and after.\n\n        Returns a list of lists:\n        [\n          [  # For file1\n             [ (line_number, text), (line_number, text), ... ],\n             [ ... ],\n          ],\n          [  # For file2\n             ...\n          ],\n          ...\n        ]\n        \"\"\"\n        all_matches_per_file: List[List[List[Tuple[int, str]]]] = []\n\n        for path, terms in search_map.items():\n            if not os.path.isfile(path):\n                # If the file is not found, record an empty list\n                all_matches_per_file.append([])\n                continue\n\n            with open(path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n                lines = f.readlines()\n\n            file_snippets: List[List[Tuple[int, str]]] = []\n            num_lines: int = len(lines)\n\n            for i, line in enumerate(lines, start=1):\n                if any(t in line for t in terms):\n                    start_idx: int = max(1, i - context_lines)\n                    end_idx: int = min(num_lines, i + context_lines)\n                    snippet: List[Tuple[int, str]] = []\n                    for snippet_no in range(start_idx, end_idx + 1):\n                        text_content: str = lines[snippet_no - 1].rstrip(\"\\n\")\n                        snippet.append((snippet_no, text_content))\n                    file_snippets.append(snippet)\n\n            all_matches_per_file.append(file_snippets)\n\n        return all_matches_per_file\n\n    # ---------------------------------------------------------\n    # 3. MERGE OVERLAPPING/ADJACENT SNIPPETS\n    # ---------------------------------------------------------\n\n    def merge_file_snippets(\n        file_snippets: List[List[Tuple[int, str]]], gap: int = 0\n    ) -> List[List[Tuple[int, str]]]:\n        \"\"\"\n        Merge overlapping or nearly adjacent snippets in a single file’s snippet list.\n        \"\"\"\n        intervals: List[Tuple[int, int, List[Tuple[int, str]]]] = []\n        for snippet in file_snippets:\n            if snippet:\n                start_line: int = snippet[0][0]\n                end_line: int = snippet[-1][0]\n                intervals.append((start_line, end_line, snippet))\n\n        intervals.sort(key=lambda x: x[0])  # sort by start line\n\n        merged: List[Tuple[int, int, List[Tuple[int, str]]]] = []\n        for start, end, snippet in intervals:\n            if not merged:\n                merged.append((start, end, snippet))\n                continue\n\n            prev_start, prev_end, prev_snippet = merged[-1]\n            if start <= prev_end + gap:\n                new_end: int = max(end, prev_end)\n                combined_dict: Dict[int, str] = {}\n                for ln, txt in prev_snippet:\n                    combined_dict[ln] = txt\n                for ln, txt in snippet:\n                    combined_dict[ln] = txt\n                merged_snippet: List[Tuple[int, str]] = [\n                    (ln, combined_dict[ln]) for ln in sorted(combined_dict)\n                ]\n                merged[-1] = (prev_start, new_end, merged_snippet)\n            else:\n                merged.append((start, end, snippet))\n\n        # Extract just the merged snippet portion\n        return [x[2] for x in merged]\n\n    def merge_all_snippets(\n        all_files_snips: List[List[List[Tuple[int, str]]]], gap: int = 0\n    ) -> List[List[List[Tuple[int, str]]]]:\n        \"\"\"\n        Merge snippet blocks within each file.\n        all_files_snips is a list-of-lists:\n          [\n            [ snippetA, snippetB, ... ],  # file 1\n            [ snippetC, snippetD, ... ],  # file 2\n          ]\n        \"\"\"\n        merged: List[List[List[Tuple[int, str]]]] = []\n        for snips in all_files_snips:\n            merged.append(merge_file_snippets(snips, gap=gap))\n        return merged\n\n    # ---------------------------------------------------------\n    # 4. RUN LOGIC: generate files, search, merge, and BUILD A STRING\n    # ---------------------------------------------------------\n\n    has_any_matches: bool = False\n\n    # 1) Gather snippets around each match\n    context_snippets: List[List[List[Tuple[int, str]]]] = (\n        find_lines_in_files_with_context(files_to_search, context_lines=context_lines)\n    )\n\n    # 2) Merge overlapping snippets\n    merged_snips: List[List[List[Tuple[int, str]]]] = merge_all_snippets(\n        context_snippets, gap=max_gap\n    )\n\n    # 3) Build a string (instead of printing)\n    output = StringIO()\n\n    # Header\n    output.write(\"Sample files created successfully.\\n\\n\")\n    output.write(\"Search Results (by file, merging any overlapping context):\\n\\n\")\n\n    # For each file\n    for (filepath, terms), snippet_list in zip(files_to_search.items(), merged_snips):\n        output.write(f\"[file name]: {filepath[len(REPO_PATH) + 1:]}\\n\")\n        terms_searched_as_str = \"\\n\".join(terms)\n        output.write(f\"[terms searched]:\\n{terms_searched_as_str}\\n\")\n        output.write(\"[file content begin]\\n\")\n        if not snippet_list:\n            output.write(\"  No matches found.\\n\")\n        else:\n            has_any_matches = True\n            for snippet_idx, snippet in enumerate(snippet_list, start=1):\n                snippet_start: int = snippet[0][0]\n                snippet_end: int = snippet[-1][0]\n                output.write(\n                    f\"\\nMatch #{snippet_idx}, lines {snippet_start} to {snippet_end}:\\n\"\n                )\n                for line_no, text in snippet:\n                    output.write(f\"  {line_no:3d} | {text}\\n\")\n                output.write(\"\\n\")\n        output.write(\"[file content end]\\n\\n\")\n\n    file_content_string: str = output.getvalue()\n\n    if has_any_matches:\n        return file_content_string\n    return \"\"","metadata":{"_uuid":"f8ae44f3-2595-4c78-b58a-6551c0fec3a8","_cell_guid":"2e6aebd6-10e8-4769-98dc-4e89aaf35b41","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-07T23:44:07.871934Z","iopub.execute_input":"2025-03-07T23:44:07.872138Z","iopub.status.idle":"2025-03-07T23:44:07.887445Z","shell.execute_reply.started":"2025-03-07T23:44:07.872121Z","shell.execute_reply":"2025-03-07T23:44:07.886837Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def fetch_file_contents(files_to_search: Dict[str, List[str]], context_lines: int = 12, max_gap: int = 0) -> str:\n    from io import StringIO\n\n    output = StringIO()\n    has_any_matches = False\n\n    for filepath, terms in files_to_search.items():\n        if not os.path.isfile(filepath):\n            continue\n        \n        with open(filepath, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n            lines = f.readlines()\n\n        file_snippets = []\n        num_lines = len(lines)\n        matched_lines = set()\n\n        # Improved token-based matching\n        for i, line in enumerate(lines, 1):\n            if any(term.lower() in line.lower() for term in terms):\n                start_idx = max(1, i - context_lines)\n                end_idx = min(len(lines), i + context_lines)\n                snippet = []\n                for snippet_no in range(start_idx, end_idx + 1):\n                    text_content = lines[snippet_no - 1].rstrip(\"\\n\")\n                    snippet.append((snippet_no, text_content))\n                file_snippets.append(snippet)\n\n        # Remove redundant snippets by line numbers\n        merged_snippets = []\n        for snippet in file_snippets:\n            if not snippet:\n                continue\n            if merged_snippets_overlap(merged_snippets=merged_snips, new_snippet=snippet, gap=max_gap):\n                merge_snippet_into_existing(merged_snips, snippet)\n            else:\n                merged_snips.append(snippet)\n\n        if merged_snips:\n            has_any_matches = True\n            output.write(f\"[file name]: {filepath[len(REPO_PATH) + 1:]}\\n\")\n            output.write(f\"[terms searched]:\\n{', '.join(terms)}\\n\")\n            output.write(\"[file content begin]\\n\")\n            for snippet_idx, snippet in enumerate(merged_snips, start=1):\n                snippet_start = snippet[0][0]\n                snippet_end = snippet[-1][0]\n                output.write(f\"\\nMatch #{snippet_idx}, lines {snippet_start} to {snippet_end}:\\n\")\n                for line_no, text in snippet:\n                    output.write(f\"  {line_no:3d} | {text}\\n\")\n                output.write(\"\\n\")\n            output.write(\"[file content end]\\n\\n\")\n\n    return output.getvalue() if has_any_matches else \"\"\n\ndef merged_snippets_overlap(snippet_a, snippet_b, gap=0):\n    return snippet[-1][0] + gap >= snippet[0][0]\n\ndef merged_snippet_into_existing(merged_snips, snippet):\n    merged_snips[-1].extend(snippet)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\n\ndef extract_patch_string(text: str) -> Optional[str]:\n    pattern: str = r\"\\n```diff\\n(.*?)\\n```\"\n    matches: List[str] = re.findall(pattern, text, re.DOTALL)\n    if not matches:\n        return None\n    return matches[-1] + \"\\n\"","metadata":{"_uuid":"5dcae870-2699-4110-956b-121c8687bfa9","_cell_guid":"a2564919-7162-4711-a5c6-92077938254f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-07T23:44:07.888108Z","iopub.execute_input":"2025-03-07T23:44:07.888309Z","iopub.status.idle":"2025-03-07T23:44:07.899594Z","shell.execute_reply.started":"2025-03-07T23:44:07.888293Z","shell.execute_reply":"2025-03-07T23:44:07.899023Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"patching_prompt: str = (\n    \"\"\"\nIssue:\n{problem_statement}\n\nRelevant Code:\n{file_content_string}\n\nInstructions:\n1. Briefly identify the root cause of the problem.\n2. Provide the necessary git diff that resolves the issue completely.\n3. Do not introduce new errors or side-effects.\n\nOutput your diff clearly wrapped within ```diff and ``` as follows:\n\n```diff\n--- a/first.txt\n+++ b/first.txt\n@@ -1,3 +1,3 @@\n start\n-first change\n+new first change\n middle\n@@ -7,4 +7,4 @@\n some content\n-second change\n+new second change\n more content\n--- a/second.txt\n+++ b/second.txt\n@@ -1,3 +1,3 @@\n beginning\n-old line\n+new line\n end\n```\n\nReminder\n- Put your diff within ```diff and ``` and make sure the diff is valid.\n- Only the last diff printed will be considered.\n\"\"\".strip()\n)\n\nimport re\n\n\ndef get_patch_string(\n    problem_statement: str, file_content_strings: List[str]\n) -> Tuple[List[str], List[Optional[str]]]:\n    sampling_params: SamplingParams = SamplingParams(\n        temperature=0.6,  # randomness of the sampling\n        min_p=0.01,\n        skip_special_tokens=True,  # Whether to skip special tokens in the output\n        max_tokens=MAX_TOKENS,\n    )\n\n    inference_idx_to_input_idx: list[int] = [\n        input_idx\n        for input_idx, file_content_string in enumerate(file_content_strings)\n        if file_content_string != \"\"\n    ]\n\n    list_of_messages: List[List[Dict[str, str]]] = [\n        [\n            {\n                \"role\": \"user\",\n                \"content\": patching_prompt.format(\n                    problem_statement=problem_statement[:20_000],\n                    file_content_string=file_content_strings[input_idx][:30_000],\n                ),\n            },\n        ]\n        for input_idx in inference_idx_to_input_idx\n    ]\n\n    prompt_texts: List[str] = [\n        (\n            tokenizer.apply_chat_template(\n                conversation=messages, tokenize=False, add_generation_prompt=True\n            )  # type: ignore\n        )\n        + \"<think>\\n\"\n        for messages in list_of_messages\n    ]\n    # print(prompt_texts)\n\n    print(\"get_patch_string\", [count_tokens(text) for text in prompt_texts])\n    request_outputs: list[RequestOutput] = llm.generate(\n        prompt_texts, sampling_params=sampling_params\n    )\n    response_texts_from_inference: List[str] = [\n        request_output.outputs[0].text for request_output in request_outputs\n    ]\n    print(\n        \"get_patch_string\",\n        [count_tokens(text) for text in response_texts_from_inference],\n    )\n    completion_texts_from_inference = [\n        prompt_text + response_text\n        for prompt_text, response_text in zip(\n            prompt_texts, response_texts_from_inference\n        )\n    ]\n    patch_strings_from_inference: List[Optional[str]] = [\n        extract_patch_string(response_text)\n        for response_text in response_texts_from_inference\n    ]\n\n    completion_texts: list[str] = [\"\" for _ in file_content_strings]\n    patch_strings: List[Optional[str]] = [None for _ in file_content_strings]\n    for inference_idx, (completion_text, patch_string) in enumerate(\n        zip(completion_texts_from_inference, patch_strings_from_inference)\n    ):\n        input_idx = inference_idx_to_input_idx[inference_idx]\n        completion_texts[input_idx] = completion_text\n        patch_strings[input_idx] = patch_string\n\n    return completion_texts, patch_strings","metadata":{"_uuid":"3229d17a-0142-4003-8b3c-ba42ae72d4d9","_cell_guid":"2000b5e2-13f3-4b86-8743-4aff8a950f2c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-07T23:44:07.900153Z","iopub.execute_input":"2025-03-07T23:44:07.900348Z","iopub.status.idle":"2025-03-07T23:44:07.910094Z","shell.execute_reply.started":"2025-03-07T23:44:07.900331Z","shell.execute_reply":"2025-03-07T23:44:07.909512Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"from pathlib import Path\n\nverifying_prompt: str = (\n    \"\"\"\nThis is the problem statement.\n\n{problem_statement}\n\nThese are the files that is thought to be relevant, which may not be complete.\n\n{file_content_string}\n\nThis is the proposed patch to fix the problem.\n\n{patch_string}\n\nEvaluate whether the patch works\n- The patch fully fixes the problem described in the problem statement.\n- The patch does not cause side effects and make any other tests fail.\n\nEnd your response with exactly either of\n- <label>Yes</label>, if absolutely confident that this fixes the problem.\n- <label>No</label>, this does not fix the problem or if you are unsure.\n\nReminder\n- Only evaluate, do not provide suggestion on how to fix.\n- Remember to write exactly either of <label>Yes</label> or <label>No</label> in the last line\n\"\"\".strip()\n)\n\n\ndef is_valid_patch_format(patch_string: str) -> bool:\n    \"\"\"\n    A quick check to confirm if a patch could be valid.\n    \"\"\"\n    if not(isinstance(patch_string, str)):\n        return False\n    try:\n        patch_set = unidiff.PatchSet(patch_string)\n        if len(patch_set) == 0:\n            return False\n    except Exception:\n        return False\n    return True\n\n\ndef patch_dry_run_succeeds(patch_string: str, repo_path: str = REPO_PATH, timeout: int = 120) -> bool:\n    \"\"\"\n    A robust check if the patch will proceed without any errors.\n    Should be run after `is_valid_patch_format()`: the patch\n    command can hang if the inputs are sufficiently invalid.\n\n    Args:\n        patch_path: Path to a file containing the patch.\n        repo_path: Path to the directory to be patched.\n        timeout: Number of seconds before the dry run will be cancelled.\n    \"\"\"\n    with open(\"patch.txt\", \"w\") as f:\n        f.write(patch_string)\n    patch_path = \"/kaggle/working/patch.txt\"\n\n    cmd = f\"patch --quiet --dry-run -p1 -i {patch_path} -d {repo_path}\"\n    try:\n        subprocess.run(cmd, shell=True, check=True, timeout=timeout)\n        return True\n    except subprocess.CalledProcessError:\n        return False\n\n\ndef get_verification(\n    problem_statement: str,\n    file_content_strings: List[str],\n    patch_strings: List[Optional[str]],\n    repo_path: str,\n) -> Tuple[List[List[str]], List[List[bool]]]:\n    assert len(file_content_strings) == len(patch_strings)\n    sampling_params: SamplingParams = SamplingParams(\n        temperature=0.6,  # randomness of the sampling\n        min_p=0.01,\n        skip_special_tokens=True,  # Whether to skip special tokens in the output\n        max_tokens=MAX_TOKENS,\n    )\n\n    inference_idx_to_input_idx: list[int] = [\n        input_idx\n        for _ in range(VALIDATION_COPY_COUNT)\n        for input_idx, patch_string in enumerate(patch_strings)\n        if patch_string is not None and is_valid_patch_format(patch_string) # and patch_dry_run_succeeds(patch_string, repo_path)\n    ]\n    print(inference_idx_to_input_idx)\n\n    list_of_messages: List[List[Dict[str, str]]] = [\n        [\n            {\n                \"role\": \"user\",\n                \"content\": verifying_prompt.format(\n                    problem_statement=problem_statement[:20_000],\n                    file_content_string=file_content_strings[input_idx][:30_000],\n                    patch_string=patch_strings[input_idx],\n                ),\n            },\n        ]\n        for input_idx in inference_idx_to_input_idx\n    ]\n\n    prompt_texts: List[str] = [\n        (\n            tokenizer.apply_chat_template(\n                conversation=messages, tokenize=False, add_generation_prompt=True\n            )  # type: ignore\n        )\n        + \"<think>\\n\"\n        for messages in list_of_messages\n    ]\n    # print(prompt_texts)\n\n    print(\"get_verification\", [count_tokens(text) for text in prompt_texts])\n    request_outputs: list[RequestOutput] = llm.generate(\n        prompt_texts, sampling_params=sampling_params\n    )\n    response_texts: List[str] = [\n        request_output.outputs[0].text for request_output in request_outputs\n    ]\n    print(\"get_verification\", [count_tokens(text) for text in response_texts])\n\n    completion_texts = [\n        prompt_text + response_text\n        for prompt_text, response_text in zip(prompt_texts, response_texts)\n    ]\n    judgments_flattened: List[bool] = [\n        \"<label>Yes</label>\" in response_text for response_text in response_texts\n    ]\n    print(judgments_flattened)\n\n    judgments_aggregated: List[List[bool]] = [[] for _ in file_content_strings]\n    completion_text_aggregated: List[List[str]] = [[] for _ in patch_strings]\n    for inference_idx, (completion_text, judgement) in enumerate(\n        zip(completion_texts, judgments_flattened)\n    ):\n        input_idx = inference_idx_to_input_idx[inference_idx]\n        completion_text_aggregated[input_idx].append(completion_text)\n        judgments_aggregated[input_idx].append(judgement)\n    print(judgments_aggregated)\n\n    return completion_text_aggregated, judgments_aggregated","metadata":{"_uuid":"5f229582-95fc-48d2-ac8e-27e71fd5e4c9","_cell_guid":"b7e7aec9-62b4-4dc2-af35-b8f01bdcff73","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-07T23:44:07.910694Z","iopub.execute_input":"2025-03-07T23:44:07.910902Z","iopub.status.idle":"2025-03-07T23:44:07.921851Z","shell.execute_reply.started":"2025-03-07T23:44:07.910872Z","shell.execute_reply":"2025-03-07T23:44:07.921275Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"import unidiff\nimport subprocess\n\n\nclass AdaptiveConfidence:\n    def __init__(self, window_size=10, min_threshold=0, max_threshold=2):\n        self.history = []\n        self.window_size = window_size\n        self.min_threshold = min_threshold\n        self.max_threshold = max_threshold\n\n    def update(self, was_correct: bool):\n        \"\"\"Update history with 1 for correct patches, 0 otherwise\"\"\"\n        self.history.append(1 if was_correct else 0)\n        if len(self.history) > self.window_size:\n            self.history.pop(0)\n\n    def get_threshold(self) -> int:\n        \"\"\"Dynamically adjust skipping threshold based on recent performance\"\"\"\n        success_rate = sum(self.history) / max(1, len(self.history))\n        if success_rate > 0.8:\n            return self.min_threshold  # Be aggressive in submissions\n        elif success_rate > 0.5:\n            return self.min_threshold + 1\n        else:\n            return self.max_threshold  # Be cautious if success rate is low\n\n\n# Global adaptive confidence tracker\nadaptive_confidence = AdaptiveConfidence()\n\n\ndef choose_patch_string(\n    patch_strings: list[Optional[str]], judgments_aggregated: List[List[bool]], repo_path: str\n) -> tuple[list[int], Optional[str]]:\n    best_score = -4\n    best_patch_string = None\n    scores = []\n\n    confidence_threshold = adaptive_confidence.get_threshold()  # Dynamically get threshold\n\n    for judgments, patch_string in zip(judgments_aggregated, patch_strings):\n\n        if patch_string is None:\n            scores.append(-3)\n            continue\n\n        if not is_valid_patch_format(patch_string):\n            scores.append(-2)\n            continue\n\n        if not patch_dry_run_succeeds(patch_string, repo_path):\n            scores.append(-1)\n            continue\n\n        score = judgments.count(True)\n        scores.append(score)\n\n        if score > best_score:\n            best_score = score\n            best_patch_string = patch_string\n\n    # **Skip issues where confidence is too low**\n    if best_score < confidence_threshold:\n        print(f\"Skipping issue due to low confidence (Threshold: {confidence_threshold})\")\n        return scores, None  # Explicitly skip the issue\n\n    # Update confidence history (if patch is chosen)\n    adaptive_confidence.update(best_score >= confidence_threshold)\n\n    return scores, best_patch_string\n","metadata":{"_uuid":"ac627a45-b48c-4bc3-b3e8-ff0ee18b3fc6","_cell_guid":"9814b304-d64c-4d61-b1d7-a4ab7b85496f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-07T23:44:07.922509Z","iopub.execute_input":"2025-03-07T23:44:07.922709Z","iopub.status.idle":"2025-03-07T23:44:07.934597Z","shell.execute_reply.started":"2025-03-07T23:44:07.922692Z","shell.execute_reply":"2025-03-07T23:44:07.934037Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"# Predict function","metadata":{"_uuid":"332ef3ee-d385-4103-8c68-d547dda443bf","_cell_guid":"d0812f8b-c4aa-47f0-a3be-36d135908ffc","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"def predict_inner(problem_statement: str, directory: str, retries: int = 2) -> Optional[str]:\n    directory_string = stringify_directory(directory)\n    \n    # Initial file selection and patch attempt\n    selection_completion_texts, file_queries = get_selection_query(directory_string, problem_statement)\n    file_content_strings = [fetch_file_contents(file_query) for file_query in file_queries]\n\n    for attempt in range(retries):\n        patch_completion_texts, patch_strings = get_patch_string(problem_statement, file_content_strings)\n\n        verification_completion_texts_aggregated, judgments_aggregated = get_verification(\n            problem_statement, file_content_strings, patch_strings, directory\n        )\n\n        scores, best_patch_string = choose_patch_string(patch_strings, judgments_aggregated)\n\n        if best_patch_string is not None:\n            if judgments_aggregated and all(judgments_aggregated[0]):\n                print(\"Patch successful on attempt\", attempt + 1)\n                return best_patch_string\n            \n            # Append feedback to the problem statement if failed\n            problem_statement += \"\\nFeedback from tests:\\n\" + summarize_test_failures(judgments_aggregated)\n\n    print(\"Patch unsuccessful after retries\")\n    return None\n\ndef summarize_test_failures(judgments):\n    feedback = []\n    for idx, judgment in enumerate(judgments):\n        if not all(judgments):\n            feedback_text = f\"Issue in verification #{idx + 1}: Patch didn't fix the issue completely.\"\n            return feedback_text\n    return \"Tests indicate unknown failure.\"\n","metadata":{"_uuid":"d5aa7692-14fa-46cb-81aa-4a05d2990df4","_cell_guid":"41d164a7-440a-4b3d-b82b-9473575c89a4","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-07T23:44:07.935075Z","iopub.execute_input":"2025-03-07T23:44:07.935290Z","iopub.status.idle":"2025-03-07T23:44:07.943580Z","shell.execute_reply.started":"2025-03-07T23:44:07.935272Z","shell.execute_reply":"2025-03-07T23:44:07.943005Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def safe_fetch(filepath):\n    try:\n        with open(filepath, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n            return f.readlines()\n    except Exception as e:\n        print(f\"Error reading {filepath}: {e}\")\n        return []\n\n# Similarly:\ndef safe_patch_apply(patch_string):\n    try:\n        with open(\"patch.txt\", \"w\") as f:\n            f.write(patch_string)\n        cmd = f\"patch --quiet --dry-run -p1 -i patch.txt -d {REPO_PATH}\"\n        subprocess.run(cmd, shell=True, check=True, timeout=10)\n        return True\n    except subprocess.CalledProcessError as e:\n        print(f\"Patch failed: {e}\")\n        return False\n    except subprocess.TimeoutExpired:\n        print(\"Patch apply timed out\")\n        return False\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import io\nfrom typing import Optional, List\n\nskip_prediction: bool = False\n\n\ndef predict(\n    problem_statement: str,\n    repo_archive: io.BytesIO,\n    pip_packages_archive: io.BytesIO,\n    env_setup_cmds_templates: List[str],\n) -> Optional[str]:\n    \"\"\"Replace this function with your inference code.\n    Args:\n        problem_statement: The text of the git issue.\n        repo_archive: A BytesIO buffer path with a .tar containing the codebase that must be patched. The gateway will make this directory available immediately before this function runs.\n    \"\"\"\n    global skip_prediction\n    if skip_prediction:\n        return None\n\n    with open(\"repo_archive.tar\", \"wb\") as f:\n        f.write(repo_archive.read())\n    repo_path: str = REPO_PATH\n    if os.path.exists(repo_path):\n        shutil.rmtree(repo_path)\n    shutil.unpack_archive(\"repo_archive.tar\", extract_dir=repo_path)\n    os.remove(\"repo_archive.tar\")\n\n    patch_string: Optional[str] = None\n    patch_string = predict_inner(\n        problem_statement=problem_statement, directory=repo_path\n    )\n    shutil.rmtree(repo_path)\n\n    if not os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n        skip_prediction = True\n\n    print(\"submitted patch_string\")\n    print(patch_string)\n\n    if patch_string is None:\n        return None\n\n    return patch_string","metadata":{"_uuid":"1fe33c0a-f4bb-4d58-a31a-8a35efed47f4","_cell_guid":"f6603ce1-36ca-4c32-b6b7-55d970a38851","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-07T23:44:07.944228Z","iopub.execute_input":"2025-03-07T23:44:07.944436Z","iopub.status.idle":"2025-03-07T23:44:07.955502Z","shell.execute_reply.started":"2025-03-07T23:44:07.944420Z","shell.execute_reply":"2025-03-07T23:44:07.954945Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"# Get predict data without server","metadata":{"_uuid":"ec5157f3-fc1a-4944-a94b-d472995e12cd","_cell_guid":"81b9a808-f41a-4715-aafc-73184d4cc947","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import os\nimport zipfile\n\n# !mkdir -p /kaggle/tmp/konwinski-prize-alt\nos.makedirs(\"/kaggle/tmp/konwinski-prize-alt\", exist_ok=True)\n\n# !unzip -q -o /kaggle/input/konwinski-prize/data.a_zip -d /kaggle/tmp/konwinski-prize-alt/ 2>/dev/null || true\ntry:\n    with zipfile.ZipFile(\"/kaggle/input/konwinski-prize/data.a_zip\", \"r\") as zip_ref:\n        zip_ref.extractall(\"/kaggle/tmp/konwinski-prize-alt/\")\nexcept:\n    pass","metadata":{"_uuid":"6d66b2c6-91e0-4652-b671-a2534be7e103","_cell_guid":"91157abf-a02f-47f2-b6b2-64057f65adaa","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-07T23:44:07.956176Z","iopub.execute_input":"2025-03-07T23:44:07.956408Z","iopub.status.idle":"2025-03-07T23:44:11.345966Z","shell.execute_reply.started":"2025-03-07T23:44:07.956388Z","shell.execute_reply":"2025-03-07T23:44:11.345216Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"import pandas as pd\n\n\ndef get_problem(problem_index: int) -> Tuple[str, str, io.BytesIO]:\n    df = pd.read_parquet(\"/kaggle/tmp/konwinski-prize-alt/data/data.parquet\")\n\n    problem_statement: str = df[\"problem_statement\"][problem_index]\n    repo_path: str = (\n        f\"/kaggle/tmp/konwinski-prize-alt/data/repos/repo__{df['instance_id'][problem_index]}\"\n    )\n\n    import shutil\n    import tempfile\n\n    with tempfile.TemporaryDirectory() as tmpdir:\n        shutil.make_archive(os.path.join(tmpdir, \"a_repo\"), \"tar\", repo_path)\n        with open(os.path.join(tmpdir, \"a_repo.tar\"), \"rb\") as f:\n            repo_archive = io.BytesIO(f.read())\n\n    return problem_statement, repo_path, repo_archive","metadata":{"_uuid":"46af49a7-33eb-4d09-9830-7c96f125cae4","_cell_guid":"eb711b99-4aff-48a8-8512-a00e18923f41","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-07T23:44:11.346695Z","iopub.execute_input":"2025-03-07T23:44:11.346942Z","iopub.status.idle":"2025-03-07T23:44:11.351414Z","shell.execute_reply.started":"2025-03-07T23:44:11.346922Z","shell.execute_reply":"2025-03-07T23:44:11.350746Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"\"\"\"demo_problem_index: int = 0\n\nif os.getenv(\"KAGGLE_KERNEL_RUN_TYPE\") == \"Interactive\" and not os.getenv(\n    \"KAGGLE_IS_COMPETITION_RERUN\"\n):\n    problem_statement, repo_path, repo_archive = get_problem(\n        problem_index=demo_problem_index\n    )\n\n    print(repo_path)\n    print(problem_statement)\n    print(len(list(repo_archive)))\n    print(len(list(repo_archive)))\"\"\"","metadata":{"_uuid":"71445695-cf75-4bdf-bf38-e9d5fdc7c07c","_cell_guid":"61fe4186-964b-4385-8837-e2f33c44c722","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-07T23:44:11.352047Z","iopub.execute_input":"2025-03-07T23:44:11.352267Z","iopub.status.idle":"2025-03-07T23:44:11.677109Z","shell.execute_reply.started":"2025-03-07T23:44:11.352248Z","shell.execute_reply":"2025-03-07T23:44:11.676426Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"'demo_problem_index: int = 0\\n\\nif os.getenv(\"KAGGLE_KERNEL_RUN_TYPE\") == \"Interactive\" and not os.getenv(\\n    \"KAGGLE_IS_COMPETITION_RERUN\"\\n):\\n    problem_statement, repo_path, repo_archive = get_problem(\\n        problem_index=demo_problem_index\\n    )\\n\\n    print(repo_path)\\n    print(problem_statement)\\n    print(len(list(repo_archive)))\\n    print(len(list(repo_archive)))'"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"\"\"\"if os.getenv(\"KAGGLE_KERNEL_RUN_TYPE\") == \"Interactive\" and not os.getenv(\n    \"KAGGLE_IS_COMPETITION_RERUN\"\n):\n    skip_prediction = False\n    problem_statement, repo_path, repo_archive = get_problem(\n        problem_index=demo_problem_index\n    )\n    patch_string = predict(problem_statement, repo_archive, io.BytesIO(), [])\"\"\"","metadata":{"_uuid":"40eac664-ddd7-462d-b9a2-6a65353e8b5a","_cell_guid":"ce7abf42-5abe-41b4-aca7-3c7302749713","trusted":true,"collapsed":false,"_kg_hide-output":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-07T23:44:11.677788Z","iopub.execute_input":"2025-03-07T23:44:11.678026Z","iopub.status.idle":"2025-03-07T23:44:11.687490Z","shell.execute_reply.started":"2025-03-07T23:44:11.678006Z","shell.execute_reply":"2025-03-07T23:44:11.686894Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"'if os.getenv(\"KAGGLE_KERNEL_RUN_TYPE\") == \"Interactive\" and not os.getenv(\\n    \"KAGGLE_IS_COMPETITION_RERUN\"\\n):\\n    skip_prediction = False\\n    problem_statement, repo_path, repo_archive = get_problem(\\n        problem_index=demo_problem_index\\n    )\\n    patch_string = predict(problem_statement, repo_archive, io.BytesIO(), [])'"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"\"\"\"if (\n    os.getenv(\"KAGGLE_KERNEL_RUN_TYPE\") == \"Interactive\"\n    and not os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\")\n    and patch_string is not None\n):\n    import polars as pl\n\n    df = pl.read_parquet(\"/kaggle/tmp/konwinski-prize-alt/data/data.parquet\")\n\n    import kaggle_evaluation.konwinski_prize_gateway\n\n    k_prize_gateway = kaggle_evaluation.konwinski_prize_gateway.KPrizeGateway()\n    k_prize_gateway.unpack_data_paths()\n\n    results = k_prize_gateway._evaluate_instance(\n        instance=df.row(demo_problem_index, named=True),\n        patch=patch_string,\n    )\n\n    from collections import Counter\n    print(\n        demo_problem_index, Counter(result.unit_test_outcome for result in results[1:])\n    )\"\"\"","metadata":{"_uuid":"c00544c0-d176-4d48-a025-6cbac48576ec","_cell_guid":"11968fc9-c39e-4088-b953-74ea01632ad3","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-07T23:44:11.688101Z","iopub.execute_input":"2025-03-07T23:44:11.688324Z","iopub.status.idle":"2025-03-07T23:44:11.698129Z","shell.execute_reply.started":"2025-03-07T23:44:11.688306Z","shell.execute_reply":"2025-03-07T23:44:11.697473Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"'if (\\n    os.getenv(\"KAGGLE_KERNEL_RUN_TYPE\") == \"Interactive\"\\n    and not os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\")\\n    and patch_string is not None\\n):\\n    import polars as pl\\n\\n    df = pl.read_parquet(\"/kaggle/tmp/konwinski-prize-alt/data/data.parquet\")\\n\\n    import kaggle_evaluation.konwinski_prize_gateway\\n\\n    k_prize_gateway = kaggle_evaluation.konwinski_prize_gateway.KPrizeGateway()\\n    k_prize_gateway.unpack_data_paths()\\n\\n    results = k_prize_gateway._evaluate_instance(\\n        instance=df.row(demo_problem_index, named=True),\\n        patch=patch_string,\\n    )\\n\\n    from collections import Counter\\n    print(\\n        demo_problem_index, Counter(result.unit_test_outcome for result in results[1:])\\n    )'"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"\"\"\"if (\n    os.getenv(\"KAGGLE_KERNEL_RUN_TYPE\") == \"Interactive\"\n    and not os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\")\n    and patch_string is not None\n):\n    from kaggle_evaluation.konwinski_prize_gateway import UnitTestOutcome\n\n    for result in results[1:]:\n        if result.unit_test_outcome != UnitTestOutcome.PASSED:\n            print(result.test_name)\n            print(result.fail_description)\"\"\"\n            ","metadata":{"_uuid":"db5f7042-adf4-42c0-918e-cdb9031cd545","_cell_guid":"74a18506-0c71-4c43-828e-ac63c9f066a0","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-07T23:44:11.698790Z","iopub.execute_input":"2025-03-07T23:44:11.699015Z","iopub.status.idle":"2025-03-07T23:44:11.710957Z","shell.execute_reply.started":"2025-03-07T23:44:11.698997Z","shell.execute_reply":"2025-03-07T23:44:11.710411Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"'if (\\n    os.getenv(\"KAGGLE_KERNEL_RUN_TYPE\") == \"Interactive\"\\n    and not os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\")\\n    and patch_string is not None\\n):\\n    from kaggle_evaluation.konwinski_prize_gateway import UnitTestOutcome\\n\\n    for result in results[1:]:\\n        if result.unit_test_outcome != UnitTestOutcome.PASSED:\\n            print(result.test_name)\\n            print(result.fail_description)'"},"metadata":{}}],"execution_count":21},{"cell_type":"markdown","source":"When your notebook is run on the hidden test set, inference_server.serve must be called within 15 minutes of the notebook starting or the gateway will throw an error. If you need more than 15 minutes to load your model you can do so during the very first predict call, which does not have the usual 30 minute response deadline.","metadata":{"_uuid":"3547dec1-18d2-49b0-8c08-ac09d8193723","_cell_guid":"3c0427bb-5570-4314-83bd-bc17d09fe3c7","trusted":true,"collapsed":false,"papermill":{"duration":0.001889,"end_time":"2024-12-11T03:22:08.856283","exception":false,"start_time":"2024-12-11T03:22:08.854394","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"# Evaluation with inference server","metadata":{"_uuid":"c6870d06-100c-4160-9cf5-311910a84007","_cell_guid":"ac0ac0e2-d8c1-4e8b-8415-ae41b202d87d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"skip_prediction = False","metadata":{"_uuid":"6a96bb2f-85cc-4b37-b044-1ecbf8422efd","_cell_guid":"697c6852-96c5-4c3f-85af-64b340b1c65c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-07T23:44:11.711589Z","iopub.execute_input":"2025-03-07T23:44:11.711786Z","iopub.status.idle":"2025-03-07T23:44:11.719944Z","shell.execute_reply.started":"2025-03-07T23:44:11.711769Z","shell.execute_reply":"2025-03-07T23:44:11.719377Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"inference_server = (\n    kaggle_evaluation.konwinski_prize_inference_server.KPrizeInferenceServer(\n        get_number_of_instances, predict\n    )\n)\n\nif os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n    inference_server.serve()\nelse:\n    inference_server.run_local_gateway(\n        data_paths=(\n            \"/kaggle/input/konwinski-prize/\",  # Path to the entire competition dataset\n            \"/kaggle/tmp/konwinski-prize/\",  # Path to a scratch directory for unpacking data.a_zip.\n        ),  # type: ignore\n        \n        use_concurrency=True,\n    )","metadata":{"_uuid":"e03ade0d-f48f-4682-888e-bd8d258767f3","_cell_guid":"bbbf2217-ac29-4d86-bffd-afe648ae1167","trusted":true,"collapsed":false,"_kg_hide-output":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-07T23:44:11.720579Z","iopub.execute_input":"2025-03-07T23:44:11.720778Z","iopub.status.idle":"2025-03-07T23:48:06.333661Z","shell.execute_reply.started":"2025-03-07T23:44:11.720762Z","shell.execute_reply":"2025-03-07T23:48:06.323969Z"}},"outputs":[{"name":"stdout","text":"Existing uv installation found. Skipping uv installation.\nInstalling Python 3.11...\nget_selection_query [4197, 4197, 4197, 4197, 4197, 4197]\n","output_type":"stream"},{"name":"stderr","text":"Processed prompts: 100%|██████████| 6/6 [01:21<00:00, 13.62s/it, est. speed input: 308.26 toks/s, output: 86.88 toks/s]\n","output_type":"stream"},{"name":"stdout","text":"get_selection_query [992, 1043, 720, 900, 1513, 1929]\nget_patch_string [1656, 895, 862, 909, 2886, 811]\n","output_type":"stream"},{"name":"stderr","text":"Processed prompts: 100%|██████████| 6/6 [02:00<00:00, 20.10s/it, est. speed input: 66.51 toks/s, output: 115.71 toks/s]\n","output_type":"stream"},{"name":"stdout","text":"get_patch_string [3736, 2897, 1030, 2531, 2246, 1511]\n[0, 2, 4]\nget_verification [1781, 940, 2949]\n","output_type":"stream"},{"name":"stderr","text":"Processed prompts: 100%|██████████| 3/3 [00:20<00:00,  6.74s/it, est. speed input: 280.29 toks/s, output: 74.30 toks/s]\n","output_type":"stream"},{"name":"stdout","text":"get_verification [571, 378, 551]\n[True, True, True]\n[[True], [], [True], [], [True], []]\nSkipping issue due to low confidence (Threshold: 1)\nsubmitted patch_string\nNone\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"","metadata":{"_uuid":"86683ecc-947a-444b-b073-352bf009e91c","_cell_guid":"b73f8a85-f747-4a0d-9598-6bf418752bc6","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}