{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaL4","dataSources":[{"sourceId":84795,"databundleVersionId":10934030,"sourceType":"competition"},{"sourceId":221096520,"sourceType":"kernelVersion"},{"sourceId":236932,"sourceType":"modelInstanceVersion","modelInstanceId":202348,"modelId":224071}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n\n# https://www.kaggle.com/competitions/ai-mathematical-olympiad-progress-prize-2/discussion/560682#3113134\nos.environ[\"TRITON_PTXAS_PATH\"] = \"/usr/local/cuda/bin/ptxas\"","metadata":{"_uuid":"38b86f00-408f-4577-b91e-890da62fd434","_cell_guid":"eec2b282-90f3-4965-a943-54cffbeb7a5b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-02-23T22:06:58.571499Z","iopub.execute_input":"2025-02-23T22:06:58.571679Z","iopub.status.idle":"2025-02-23T22:06:58.574972Z","shell.execute_reply.started":"2025-02-23T22:06:58.571660Z","shell.execute_reply":"2025-02-23T22:06:58.574258Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import io\nimport time\nimport shutil\n\nimport pandas as pd\nimport polars as pl\n\nimport kaggle_evaluation.konwinski_prize_inference_server\nfrom typing import List, Tuple, Dict, Optional\n\nstart_time = time.time()","metadata":{"_uuid":"e9ab5832-846d-4fb5-b21b-532f5962301d","_cell_guid":"2512bd4d-9caf-4c50-9706-0377f47811ae","trusted":true,"collapsed":false,"_kg_hide-output":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-02-23T22:06:58.575905Z","iopub.execute_input":"2025-02-23T22:06:58.576117Z","iopub.status.idle":"2025-02-23T22:07:12.405192Z","shell.execute_reply.started":"2025-02-23T22:06:58.576093Z","shell.execute_reply":"2025-02-23T22:07:12.404479Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"The evaluation API requires that you set up a server which will respond to inference requests. We have already defined the server; you just need write the predict function. When we evaluate your submission on the hidden test set the client defined in `konwinski_prize_gateway` will run in a different container with direct access to the hidden test set and hand off the data.\n#\nYour code will always have access to the published copies of the files.","metadata":{"_uuid":"acfccb6c-54ff-466d-b3ca-dc9d610e1562","_cell_guid":"9b41a507-839f-49ba-bed3-7ec21f5bd402","trusted":true,"collapsed":false,"papermill":{"duration":0.002032,"end_time":"2024-12-11T03:22:08.823897","exception":false,"start_time":"2024-12-11T03:22:08.821865","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"instance_count: Optional[int] = None\n\n\ndef get_number_of_instances(num_instances: int) -> None:\n    \"\"\"The very first message from the gateway will be the total number of instances to be served.\n    You don't need to edit this function.\n    \"\"\"\n    global instance_count\n    instance_count = num_instances","metadata":{"_uuid":"f251dce7-62ae-4856-9c05-f5ffe2938eaf","_cell_guid":"7fce3650-d66d-4189-ac61-51f4e07ac289","trusted":true,"collapsed":false,"papermill":{"duration":0.011949,"end_time":"2024-12-11T03:22:08.838279","exception":false,"start_time":"2024-12-11T03:22:08.82633","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-02-23T22:07:12.406222Z","iopub.execute_input":"2025-02-23T22:07:12.406814Z","iopub.status.idle":"2025-02-23T22:07:12.410228Z","shell.execute_reply.started":"2025-02-23T22:07:12.406790Z","shell.execute_reply":"2025-02-23T22:07:12.409596Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Initialize LLM","metadata":{"_uuid":"370e67e3-a871-42b0-b0a3-ab7bbcf87dcc","_cell_guid":"e00a129e-16b7-408e-a131-75c4c476b300","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"from vllm import LLM, SamplingParams, RequestOutput\nimport warnings\n\nwarnings.simplefilter(\"ignore\")\n\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nif os.getenv(\"KAGGLE_KERNEL_RUN_TYPE\") or os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n    llm_model_pth: str = (\n        \"/kaggle/input/m/shelterw/deepseek-r1/transformers/deepseek-r1-distill-qwen-14b-awq/1\"\n        #\"/kaggle/input/mistral-small-24b/transformers/mistral-small-24b-base-2501/1\"\n        #\"/kaggle/input/deepseek-r1/transformers/deepseek-r1-distill-qwen-14b/2\"\n        #\"/kaggle/input/m/shelterw/deepseek-r1/transformers/deepseek-r1-distill-qwen-32b-awq/1\"\n    )\n# else:\n#     llm_model_pth: str = \"/root/volume/KirillR/QwQ-32B-Preview-AWQ\"\n\n\nBATCH_SIZE: int = 6\nVALIDATION_COPY_COUNT: int = 1\nMAX_TOKENS: int = 4096\n\nMAX_NUM_SEQS: int = 6\nMAX_MODEL_LEN: int = 32_768\n\nllm: LLM = LLM(\n    llm_model_pth,\n    max_num_seqs=MAX_NUM_SEQS,  # Maximum number of sequences per iteration. Default is 256\n    max_model_len=MAX_MODEL_LEN,  # Model context length\n    trust_remote_code=True,  # Trust remote code (e.g., from HuggingFace) when downloading the model and tokenizer\n    tensor_parallel_size=4,  # The number of GPUs to use for distributed execution with tensor parallelism\n    gpu_memory_utilization=0.95,  # The ratio (between 0 and 1) of GPU memory to reserve for the model\n    seed=42,#seed=2024,\n)","metadata":{"_uuid":"eca28397-2a40-4265-b36e-dfa9d80eb2b1","_cell_guid":"c863b090-e04a-441e-9add-bddae0863ef9","trusted":true,"collapsed":false,"_kg_hide-output":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-02-23T22:22:04.480413Z","iopub.execute_input":"2025-02-23T22:22:04.480746Z","execution_failed":"2025-02-23T22:22:08.386Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer = llm.get_tokenizer()\n\n\ndef count_tokens(text: str) -> int:\n    return len(tokenizer.encode(text))","metadata":{"_uuid":"5a982fe4-e4e2-466d-b5bf-4839ec8804c8","_cell_guid":"99e40ae3-6474-4b05-a969-0b48de303ba6","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-02-23T22:13:54.472650Z","iopub.execute_input":"2025-02-23T22:13:54.472894Z","iopub.status.idle":"2025-02-23T22:13:54.476195Z","shell.execute_reply.started":"2025-02-23T22:13:54.472872Z","shell.execute_reply":"2025-02-23T22:13:54.475603Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# Helper functions","metadata":{"_uuid":"30f587b9-7b4f-4812-82f7-d87787c58d1c","_cell_guid":"3a54e069-fe31-4873-9d25-a02c38576c59","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import os\n\n\ndef stringify_directory(directory: str) -> str:\n    full_paths: List[str] = []\n\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            full_path: str = os.path.join(root, file)\n            full_paths.append(full_path)\n    return \"\\n\".join(full_paths)","metadata":{"_uuid":"43546fee-8473-4f0d-80ec-46a86369426b","_cell_guid":"44d40acd-8428-4813-ba5d-62f625309dd4","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-02-23T22:13:54.477821Z","iopub.execute_input":"2025-02-23T22:13:54.478044Z","iopub.status.idle":"2025-02-23T22:13:54.492735Z","shell.execute_reply.started":"2025-02-23T22:13:54.478026Z","shell.execute_reply":"2025-02-23T22:13:54.492171Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import re\n\n\ndef extract_file_query(xml_content: str) -> Dict[str, List[str]]:\n    import xml.etree.ElementTree as ET\n\n    # Prepare a data structure to collect results\n    parsed_data: Dict[str, List[str]] = {}\n    pattern: str = r\"<root>(.*?)</root>\"\n    matches: List[str] = re.findall(pattern, xml_content, re.DOTALL)\n\n    for match in matches:\n        try:\n            # Parse the XML\n            root = ET.fromstring(\"<root>\" + match + \"</root>\")\n\n            # Find all <entry> elements\n            for entry in root.findall(\"entry\"):\n                # Extract the <filepath> text\n                filepath = entry.find(\"filepath\")\n                filepath_text: Optional[str] = (\n                    filepath.text.strip()\n                    if filepath is not None and filepath.text is not None\n                    else None\n                )\n\n                # Locate <strings_to_search> container\n                strings_container = entry.find(\"strings_to_search\")\n\n                # Gather each <string_to_search> text\n                search_strings: List[str] = []\n                if strings_container is not None:\n                    for s in strings_container.findall(\"string_to_search\"):\n                        if s.text is not None:\n                            search_strings.append(s.text.strip())\n\n                # Store in a dictionary: { filepath: [search_strings...] }\n                parsed_data[filepath_text] = search_strings  # type: ignore\n        except:\n            print(\"Error parsing output\")\n            print(xml_content)\n            return {}\n\n    return parsed_data","metadata":{"_uuid":"47b13790-f9e8-47cd-8b78-c93567e04b56","_cell_guid":"50b46124-4395-4783-b523-f11f4b6228bf","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-02-23T22:13:54.493787Z","iopub.execute_input":"2025-02-23T22:13:54.493986Z","iopub.status.idle":"2025-02-23T22:13:54.503061Z","shell.execute_reply.started":"2025-02-23T22:13:54.493968Z","shell.execute_reply":"2025-02-23T22:13:54.502488Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"reading_prompt: str = (\n    \"\"\"\nYou will be implementing a git diff patch to solve an issue with the code repository.\nYou will first need to select files in the file directory.\n\nThis is the problem statement.\n\n{problem_statement}\n\nThis is the file directory\n\n<directory>\n{directory_string}\n</directory>\n\nWhich files should be inspected so that we can solve the problem?\nWhen we inspect each file, what strings should be searched?\n\nReturn the strings to search in this format\n\n(explanation)\n\n<root>\n    <entry>\n        <filepath>filepath</filepath>  \n        <strings_to_search>\n            <string_to_search>string_to_search</string_to_search>\n            ...\n            <string_to_search>string_to_search</string_to_search>\n        </strings_to_search>\n    </entry>\n    <entry>\n        <filepath>filepath</filepath>\n        <strings_to_search>\n            <string_to_search>string_to_search</string_to_search>\n            ...\n            <string_to_search>string_to_search</string_to_search>\n        </strings_to_search>\n    </entry>\n    ...\n</root>\n...\n\nNotes:\n- Make sure to encode each entry between <root> and </root>\n- Return the FULL filepath - exactly as specified in <directory> and </directory>\n    - Example: <filepath>repo/path/to/directory/file.py</filepath>\n- If you are searching for a word instead of a substring, maybe add spaces or brackets before and after the string\n    - For example, if you are searching for uses of the function `calculate`, use ` calculate(` as the search string instead of `calculate`\n- Prefer searching longer strings\n    - Avoid searching for strings that might appear in many parts of the codebase\n- Search the test files as well to understand the feature behavior\n    - Also search for the relevant function calls in the test files\n\"\"\".strip()\n)\n\n\ndef get_selection_query(\n    directory_string: str, problem_statement: str\n) -> Tuple[List[str], List[Dict[str, List[str]]]]:\n    sampling_params: SamplingParams = SamplingParams(\n        temperature=0.6,  # randomness of the sampling\n        min_p=0.01,\n        skip_special_tokens=True,  # Whether to skip special tokens in the output\n        max_tokens=MAX_TOKENS,\n    )\n\n    list_of_messages: List[List[Dict[str, str]]] = [\n        [\n            {\n                \"role\": \"user\",\n                \"content\": reading_prompt.format(\n                    problem_statement=problem_statement[:20_000],\n                    directory_string=directory_string[:30_000],\n                ),\n            },\n        ]\n        for _ in range(BATCH_SIZE)\n    ]\n\n    prompt_texts: List[str] = [\n        (\n            tokenizer.apply_chat_template(\n                conversation=messages, tokenize=False, add_generation_prompt=True\n            )  # type: ignore\n        )\n        + \"<think>\\n\"\n        for messages in list_of_messages\n    ]\n    # print(prompt_texts)\n\n    print(\"get_selection_query\", [count_tokens(text) for text in prompt_texts])\n    request_outputs: list[RequestOutput] = llm.generate(\n        prompt_texts, sampling_params=sampling_params\n    )\n    if not request_outputs:\n        return [], []\n    response_texts: List[str] = [\n        request_output.outputs[0].text for request_output in request_outputs\n    ]\n    print(\"get_selection_query\", [count_tokens(text) for text in response_texts])\n\n    completion_texts = [\n        prompt_text + response_text\n        for prompt_text, response_text in zip(prompt_texts, response_texts)\n    ]\n    file_queries: List[Dict[str, List[str]]] = [\n        extract_file_query(response_text) for response_text in response_texts\n    ]\n    return completion_texts, file_queries","metadata":{"_uuid":"b6264935-32cf-49b5-b1da-aece43dbf184","_cell_guid":"a636bf5a-ec3a-4113-8504-bc95550036ef","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-02-23T22:13:54.503646Z","iopub.execute_input":"2025-02-23T22:13:54.503842Z","iopub.status.idle":"2025-02-23T22:13:54.511249Z","shell.execute_reply.started":"2025-02-23T22:13:54.503825Z","shell.execute_reply":"2025-02-23T22:13:54.510695Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"REPO_PATH: str = \"repo\"\n\n\ndef fetch_file_contents(\n    files_to_search: Dict[str, List[str]], context_lines: int = 12, max_gap: int = 0\n) -> str:\n    from io import StringIO\n    from typing import Tuple\n\n    def find_lines_in_files_with_context(\n        search_map: Dict[str, List[str]], context_lines: int = context_lines\n    ) -> List[List[List[Tuple[int, str]]]]:\n        \"\"\"\n        Given a dictionary mapping file paths to a list of search terms,\n        open each file and gather *snippets* of lines that contain any\n        of those search terms, including 'context_lines' before and after.\n\n        Returns a list of lists:\n        [\n          [  # For file1\n             [ (line_number, text), (line_number, text), ... ],\n             [ ... ],\n          ],\n          [  # For file2\n             ...\n          ],\n          ...\n        ]\n        \"\"\"\n        all_matches_per_file: List[List[List[Tuple[int, str]]]] = []\n\n        for path, terms in search_map.items():\n            if not os.path.isfile(path):\n                # If the file is not found, record an empty list\n                all_matches_per_file.append([])\n                continue\n\n            with open(path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n                lines = f.readlines()\n\n            file_snippets: List[List[Tuple[int, str]]] = []\n            num_lines: int = len(lines)\n\n            for i, line in enumerate(lines, start=1):\n                if any(t in line for t in terms):\n                    start_idx: int = max(1, i - context_lines)\n                    end_idx: int = min(num_lines, i + context_lines)\n                    snippet: List[Tuple[int, str]] = []\n                    for snippet_no in range(start_idx, end_idx + 1):\n                        text_content: str = lines[snippet_no - 1].rstrip(\"\\n\")\n                        snippet.append((snippet_no, text_content))\n                    file_snippets.append(snippet)\n\n            all_matches_per_file.append(file_snippets)\n\n        return all_matches_per_file\n\n    # ---------------------------------------------------------\n    # 3. MERGE OVERLAPPING/ADJACENT SNIPPETS\n    # ---------------------------------------------------------\n\n    def merge_file_snippets(\n        file_snippets: List[List[Tuple[int, str]]], gap: int = 0\n    ) -> List[List[Tuple[int, str]]]:\n        \"\"\"\n        Merge overlapping or nearly adjacent snippets in a single fileâ€™s snippet list.\n        \"\"\"\n        intervals: List[Tuple[int, int, List[Tuple[int, str]]]] = []\n        for snippet in file_snippets:\n            if snippet:\n                start_line: int = snippet[0][0]\n                end_line: int = snippet[-1][0]\n                intervals.append((start_line, end_line, snippet))\n\n        intervals.sort(key=lambda x: x[0])  # sort by start line\n\n        merged: List[Tuple[int, int, List[Tuple[int, str]]]] = []\n        for start, end, snippet in intervals:\n            if not merged:\n                merged.append((start, end, snippet))\n                continue\n\n            prev_start, prev_end, prev_snippet = merged[-1]\n            if start <= prev_end + gap:\n                new_end: int = max(end, prev_end)\n                combined_dict: Dict[int, str] = {}\n                for ln, txt in prev_snippet:\n                    combined_dict[ln] = txt\n                for ln, txt in snippet:\n                    combined_dict[ln] = txt\n                merged_snippet: List[Tuple[int, str]] = [\n                    (ln, combined_dict[ln]) for ln in sorted(combined_dict)\n                ]\n                merged[-1] = (prev_start, new_end, merged_snippet)\n            else:\n                merged.append((start, end, snippet))\n\n        # Extract just the merged snippet portion\n        return [x[2] for x in merged]\n\n    def merge_all_snippets(\n        all_files_snips: List[List[List[Tuple[int, str]]]], gap: int = 0\n    ) -> List[List[List[Tuple[int, str]]]]:\n        \"\"\"\n        Merge snippet blocks within each file.\n        all_files_snips is a list-of-lists:\n          [\n            [ snippetA, snippetB, ... ],  # file 1\n            [ snippetC, snippetD, ... ],  # file 2\n          ]\n        \"\"\"\n        merged: List[List[List[Tuple[int, str]]]] = []\n        for snips in all_files_snips:\n            merged.append(merge_file_snippets(snips, gap=gap))\n        return merged\n\n    # ---------------------------------------------------------\n    # 4. RUN LOGIC: generate files, search, merge, and BUILD A STRING\n    # ---------------------------------------------------------\n\n    has_any_matches: bool = False\n\n    # 1) Gather snippets around each match\n    context_snippets: List[List[List[Tuple[int, str]]]] = (\n        find_lines_in_files_with_context(files_to_search, context_lines=context_lines)\n    )\n\n    # 2) Merge overlapping snippets\n    merged_snips: List[List[List[Tuple[int, str]]]] = merge_all_snippets(\n        context_snippets, gap=max_gap\n    )\n\n    # 3) Build a string (instead of printing)\n    output = StringIO()\n\n    # Header\n    output.write(\"Sample files created successfully.\\n\\n\")\n    output.write(\"Search Results (by file, merging any overlapping context):\\n\\n\")\n\n    # For each file\n    for (filepath, terms), snippet_list in zip(files_to_search.items(), merged_snips):\n        output.write(f\"[file name]: {filepath[len(REPO_PATH) + 1:]}\\n\")\n        terms_searched_as_str = \"\\n\".join(terms)\n        output.write(f\"[terms searched]:\\n{terms_searched_as_str}\\n\")\n        output.write(\"[file content begin]\\n\")\n        if not snippet_list:\n            output.write(\"  No matches found.\\n\")\n        else:\n            has_any_matches = True\n            for snippet_idx, snippet in enumerate(snippet_list, start=1):\n                snippet_start: int = snippet[0][0]\n                snippet_end: int = snippet[-1][0]\n                output.write(\n                    f\"\\nMatch #{snippet_idx}, lines {snippet_start} to {snippet_end}:\\n\"\n                )\n                for line_no, text in snippet:\n                    output.write(f\"  {line_no:3d} | {text}\\n\")\n                output.write(\"\\n\")\n        output.write(\"[file content end]\\n\\n\")\n\n    file_content_string: str = output.getvalue()\n\n    if has_any_matches:\n        return file_content_string\n    return \"\"","metadata":{"_uuid":"f8ae44f3-2595-4c78-b58a-6551c0fec3a8","_cell_guid":"2e6aebd6-10e8-4769-98dc-4e89aaf35b41","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-02-23T22:13:54.511916Z","iopub.execute_input":"2025-02-23T22:13:54.512131Z","iopub.status.idle":"2025-02-23T22:13:54.527732Z","shell.execute_reply.started":"2025-02-23T22:13:54.512113Z","shell.execute_reply":"2025-02-23T22:13:54.527146Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import re\n\ndef extract_patch_string(text: str) -> Optional[str]:\n    pattern: str = r\"\\n```diff\\n(.*?)\\n```\"\n    matches: List[str] = re.findall(pattern, text, re.DOTALL)\n    if not matches:\n        return None\n    return matches[-1] + \"\\n\"","metadata":{"_uuid":"5dcae870-2699-4110-956b-121c8687bfa9","_cell_guid":"a2564919-7162-4711-a5c6-92077938254f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-02-23T22:13:54.528398Z","iopub.execute_input":"2025-02-23T22:13:54.528596Z","iopub.status.idle":"2025-02-23T22:13:54.540681Z","shell.execute_reply.started":"2025-02-23T22:13:54.528579Z","shell.execute_reply":"2025-02-23T22:13:54.540101Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"patching_prompt: str = (\n    \"\"\"\nYou will be implementing a git diff patch to solve an issue with the code repository.\nThis is the problem statement.\n\n{problem_statement}\n\nThese are the files that is thought to be relevant\n\n{file_content_string}\n\nWrite a git diff within ```diff and ``` that fully fixes the problem.\nThe git diff should not cause other tests to fail.\n\nExample:\n\n```diff\n--- a/first.txt\n+++ b/first.txt\n@@ -1,3 +1,3 @@\n start\n-first change\n+new first change\n middle\n@@ -7,4 +7,4 @@\n some content\n-second change\n+new second change\n more content\n--- a/second.txt\n+++ b/second.txt\n@@ -1,3 +1,3 @@\n beginning\n-old line\n+new line\n end\n```\n\nReminder\n- Put your diff within ```diff and ``` and make sure the diff is valid.\n- Only the last diff printed will be considered.\n\"\"\".strip()\n)\n\nimport re\n\n\ndef get_patch_string(\n    problem_statement: str, file_content_strings: List[str], num_candidates: int = 3\n) -> Tuple[List[str], List[Optional[str]]]:\n    \"\"\"\n    Generates patch candidates using diverse sampling by varying the sampling temperature.\n    Returns:\n      - A list of combined completion texts (one per file)\n      - A list of patch strings (one per file), selected from the candidate pool.\n    \"\"\"\n    # Identify indices for non-empty file content strings.\n    inference_idx_to_input_idx: List[int] = [\n        idx for idx, fc in enumerate(file_content_strings) if fc != \"\"\n    ]\n    \n    # Prepare the messages for each file.\n    list_of_messages: List[List[Dict[str, str]]] = [\n        [\n            {\n                \"role\": \"user\",\n                \"content\": patching_prompt.format(\n                    problem_statement=problem_statement[:20_000],\n                    file_content_string=file_content_strings[idx][:30_000],\n                ),\n            },\n        ]\n        for idx in inference_idx_to_input_idx\n    ]\n    \n    # Initialize dictionaries to store candidates for each file.\n    candidate_completion_texts = {idx: [] for idx in inference_idx_to_input_idx}\n    candidate_patch_strings = {idx: [] for idx in inference_idx_to_input_idx}\n    \n    # Loop to generate multiple candidates per input.\n    for candidate in range(num_candidates):\n        # Vary the temperature slightly for diversity.\n        candidate_sampling_params = SamplingParams(\n            temperature=0.6 + candidate * 0.1,\n            min_p=0.01,\n            skip_special_tokens=True,\n            max_tokens=MAX_TOKENS,\n        )\n        \n        # Build prompt texts for all messages.\n        prompt_texts: List[str] = [\n            tokenizer.apply_chat_template(\n                conversation=messages, tokenize=False, add_generation_prompt=True\n            ) + \"<think>\\n\"\n            for messages in list_of_messages\n        ]\n        \n        print(\"get_patch_string (candidate {}):\".format(candidate), [count_tokens(text) for text in prompt_texts])\n        \n        # Generate responses for this candidate round.\n        request_outputs: List[RequestOutput] = llm.generate(\n            prompt_texts, sampling_params=candidate_sampling_params\n        )\n        response_texts_from_inference: List[str] = [\n            request_output.outputs[0].text for request_output in request_outputs\n        ]\n        print(\"Candidate {} responses:\".format(candidate), [count_tokens(text) for text in response_texts_from_inference])\n        \n        # Combine prompt and response, then extract patch string.\n        completion_texts_from_inference = [\n            prompt_text + response_text\n            for prompt_text, response_text in zip(prompt_texts, response_texts_from_inference)\n        ]\n        patch_strings_from_inference: List[Optional[str]] = [\n            extract_patch_string(response_text)\n            for response_text in response_texts_from_inference\n        ]\n        \n        # Save the candidate outputs per file.\n        for idx, (comp_text, patch_str) in zip(inference_idx_to_input_idx, \n                                                 zip(completion_texts_from_inference, patch_strings_from_inference)):\n            candidate_completion_texts[idx].append(comp_text)\n            candidate_patch_strings[idx].append(patch_str)\n    \n    # For each file, select the first candidate with a valid (non-None) patch.\n    final_completion_texts: List[str] = [\"\" for _ in file_content_strings]\n    final_patch_strings: List[Optional[str]] = [None for _ in file_content_strings]\n    \n    for idx in inference_idx_to_input_idx:\n        selected_patch = None\n        selected_completion = \"\"\n        for comp_text, patch_str in zip(candidate_completion_texts[idx], candidate_patch_strings[idx]):\n            if patch_str is not None:\n                selected_patch = patch_str\n                selected_completion = comp_text\n                break\n        final_patch_strings[idx] = selected_patch\n        final_completion_texts[idx] = selected_completion\n\n    return final_completion_texts, final_patch_strings\n","metadata":{"_uuid":"3229d17a-0142-4003-8b3c-ba42ae72d4d9","_cell_guid":"2000b5e2-13f3-4b86-8743-4aff8a950f2c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-02-23T22:13:54.541326Z","iopub.execute_input":"2025-02-23T22:13:54.541529Z","iopub.status.idle":"2025-02-23T22:13:54.555763Z","shell.execute_reply.started":"2025-02-23T22:13:54.541512Z","shell.execute_reply":"2025-02-23T22:13:54.555188Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"from pathlib import Path\n\nverifying_prompt: str = (\n    \"\"\"\nThis is the problem statement.\n\n{problem_statement}\n\nThese are the files that is thought to be relevant, which may not be complete.\n\n{file_content_string}\n\nThis is the proposed patch to fix the problem.\n\n{patch_string}\n\nEvaluate whether the patch works\n- The patch fully fixes the problem described in the problem statement.\n- The patch does not cause side effects and make any other tests fail.\n\nEnd your response with exactly either of\n- <label>Yes</label>, this fixes the problem.\n- <label>No</label>, this does not fix the problem.\n\nReminder\n- Only evaluate, do not provide suggestion on how to fix.\n- Remember to write exactly either of <label>Yes</label> or <label>No</label> in the last line\n\"\"\".strip()\n)\n\n\ndef is_valid_patch_format(patch_string: str) -> bool:\n    \"\"\"\n    A quick check to confirm if a patch could be valid.\n    \"\"\"\n    if not(isinstance(patch_string, str)):\n        return False\n    try:\n        patch_set = unidiff.PatchSet(patch_string)\n        if len(patch_set) == 0:\n            return False\n    except Exception:\n        return False\n    return True\n\n\ndef patch_dry_run_succeeds(patch_string: str, repo_path: str = REPO_PATH, timeout: int = 60) -> bool:\n    \"\"\"\n    A robust check if the patch will proceed without any errors.\n    Should be run after `is_valid_patch_format()`: the patch\n    command can hang if the inputs are sufficiently invalid.\n\n    Args:\n        patch_path: Path to a file containing the patch.\n        repo_path: Path to the directory to be patched.\n        timeout: Number of seconds before the dry run will be cancelled.\n    \"\"\"\n    with open(\"patch.txt\", \"w\") as f:\n        f.write(patch_string)\n    patch_path = \"/kaggle/working/patch.txt\"\n\n    cmd = f\"patch --quiet --dry-run -p1 -i {patch_path} -d {repo_path}\"\n    try:\n        subprocess.run(cmd, shell=True, check=True, timeout=timeout)\n        return True\n    except subprocess.CalledProcessError:\n        return False\n\n\ndef get_verification(\n    problem_statement: str,\n    file_content_strings: List[str],\n    patch_strings: List[Optional[str]],\n    repo_path: str,\n) -> Tuple[List[List[str]], List[List[bool]]]:\n    assert len(file_content_strings) == len(patch_strings)\n    sampling_params: SamplingParams = SamplingParams(\n        temperature=0.6,  # randomness of the sampling\n        min_p=0.01,\n        skip_special_tokens=True,  # Whether to skip special tokens in the output\n        max_tokens=MAX_TOKENS,\n    )\n\n    inference_idx_to_input_idx: list[int] = [\n        input_idx\n        for _ in range(VALIDATION_COPY_COUNT)\n        for input_idx, patch_string in enumerate(patch_strings)\n        if patch_string is not None and is_valid_patch_format(patch_string) # and patch_dry_run_succeeds(patch_string, repo_path)\n    ]\n    print(inference_idx_to_input_idx)\n\n    list_of_messages: List[List[Dict[str, str]]] = [\n        [\n            {\n                \"role\": \"user\",\n                \"content\": verifying_prompt.format(\n                    problem_statement=problem_statement[:20_000],\n                    file_content_string=file_content_strings[input_idx][:30_000],\n                    patch_string=patch_strings[input_idx],\n                ),\n            },\n        ]\n        for input_idx in inference_idx_to_input_idx\n    ]\n\n    prompt_texts: List[str] = [\n        (\n            tokenizer.apply_chat_template(\n                conversation=messages, tokenize=False, add_generation_prompt=True\n            )  # type: ignore\n        )\n        + \"<think>\\n\"\n        for messages in list_of_messages\n    ]\n    # print(prompt_texts)\n\n    print(\"get_verification\", [count_tokens(text) for text in prompt_texts])\n    request_outputs: list[RequestOutput] = llm.generate(\n        prompt_texts, sampling_params=sampling_params\n    )\n    response_texts: List[str] = [\n        request_output.outputs[0].text for request_output in request_outputs\n    ]\n    print(\"get_verification\", [count_tokens(text) for text in response_texts])\n\n    completion_texts = [\n        prompt_text + response_text\n        for prompt_text, response_text in zip(prompt_texts, response_texts)\n    ]\n    judgments_flattened: List[bool] = [\n        \"<label>Yes</label>\" in response_text for response_text in response_texts\n    ]\n    print(judgments_flattened)\n\n    judgments_aggregated: List[List[bool]] = [[] for _ in file_content_strings]\n    completion_text_aggregated: List[List[str]] = [[] for _ in patch_strings]\n    for inference_idx, (completion_text, judgement) in enumerate(\n        zip(completion_texts, judgments_flattened)\n    ):\n        input_idx = inference_idx_to_input_idx[inference_idx]\n        completion_text_aggregated[input_idx].append(completion_text)\n        judgments_aggregated[input_idx].append(judgement)\n    print(judgments_aggregated)\n\n    return completion_text_aggregated, judgments_aggregated","metadata":{"_uuid":"5f229582-95fc-48d2-ac8e-27e71fd5e4c9","_cell_guid":"b7e7aec9-62b4-4dc2-af35-b8f01bdcff73","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-02-23T22:13:54.556471Z","iopub.execute_input":"2025-02-23T22:13:54.556680Z","iopub.status.idle":"2025-02-23T22:13:54.569435Z","shell.execute_reply.started":"2025-02-23T22:13:54.556662Z","shell.execute_reply":"2025-02-23T22:13:54.568890Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"import unidiff\nimport subprocess\n\n\ndef choose_patch_string(\n    patch_strings: list[Optional[str]], judgments_aggregated: List[List[bool]], repo_path: str\n) -> tuple[list[int], Optional[str]]:\n    best_score = -4\n    best_patch_string = None\n\n    scores = []\n    for judgments, patch_string in zip(judgments_aggregated, patch_strings):\n\n        if patch_string is None:\n            score = -3\n            scores.append(score)\n            continue\n        \n        if not is_valid_patch_format(patch_string):\n            score = -2\n            scores.append(score)\n            continue\n        \n        if not patch_dry_run_succeeds(patch_string, repo_path):\n            score = -1\n            scores.append(score)\n            continue\n        \n        score = judgments.count(True)\n        scores.append(score)\n        \n        if score > best_score:\n            best_score = score\n            best_patch_string = patch_string\n\n    return scores, best_patch_string","metadata":{"_uuid":"ac627a45-b48c-4bc3-b3e8-ff0ee18b3fc6","_cell_guid":"9814b304-d64c-4d61-b1d7-a4ab7b85496f","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-23T22:13:54.570074Z","iopub.execute_input":"2025-02-23T22:13:54.570272Z","iopub.status.idle":"2025-02-23T22:13:54.581650Z","shell.execute_reply.started":"2025-02-23T22:13:54.570255Z","shell.execute_reply":"2025-02-23T22:13:54.581062Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"# Predict function","metadata":{"_uuid":"332ef3ee-d385-4103-8c68-d547dda443bf","_cell_guid":"d0812f8b-c4aa-47f0-a3be-36d135908ffc","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"def predict_inner(problem_statement: str, directory: str) -> Optional[str]:\n    directory_string = stringify_directory(directory)\n    \n    # Run file selection once\n    selection_completion_texts, file_queries = get_selection_query(\n        directory_string, problem_statement\n    )\n    file_content_strings: List[str] = [\n        fetch_file_contents(file_query) for file_query in file_queries\n    ]\n    \n    max_iterations = 3  # Maximum number of refinement iterations\n    best_patch = None\n    best_score = -float(\"inf\")\n    final_iteration_data = None  # Will hold data from the best iteration\n    \n    # Iterative refinement loop\n    for iteration in range(max_iterations):\n        print(f\"Iteration {iteration + 1} of {max_iterations}\")\n        # Generate patch candidates (you might also adjust sampling parameters here)\n        patch_completion_texts, patch_strings = get_patch_string(\n            problem_statement, file_content_strings\n        )\n        # Verify the generated patches\n        verification_completion_texts_aggregated, judgments_aggregated = get_verification(\n            problem_statement, file_content_strings, patch_strings, directory\n        )\n        # Choose the best patch from the current iteration\n        scores, patch_candidate = choose_patch_string(patch_strings, judgments_aggregated, directory)\n        \n        # For simplicity, we use the maximum score from this iteration (could also use an average)\n        iteration_best = max(scores) if scores else -float(\"inf\")\n        print(f\"Iteration {iteration + 1} best score: {iteration_best}\")\n        \n        # Update our best candidate if this iteration is better\n        if iteration_best > best_score:\n            best_score = iteration_best\n            best_patch = patch_candidate\n            final_iteration_data = {\n                \"patch_completion_text\": patch_completion_texts,\n                \"patch_strings\": patch_strings,\n                \"scores\": scores,\n                \"judgments_aggregated\": judgments_aggregated,\n            }\n        \n        # Break early if a satisfactory patch is found (e.g., at least one positive judgment)\n        if best_score >= 1:\n            print(\"Satisfactory patch found; breaking early.\")\n            break\n\n    # Logging the iteration data if not in a competition rerun.\n    if not os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\") and final_iteration_data is not None:\n        data = {\n            \"problem_statement\": [problem_statement] * len(file_queries),\n            \"selection_completion_text\": selection_completion_texts,\n            \"selection_completion_length\": [\n                count_tokens(text) for text in selection_completion_texts\n            ],\n            \"file_query\": file_queries,\n            \"file_content_string\": file_content_strings,\n            \"patch_completion_text\": final_iteration_data[\"patch_completion_text\"],\n            \"patch_completion_length\": [\n                count_tokens(text) for text in final_iteration_data[\"patch_completion_text\"]\n            ],\n            \"patch_string\": final_iteration_data[\"patch_strings\"],\n            \"score\": final_iteration_data[\"scores\"],\n            \"best_score\": best_score,\n        }\n        # Optionally log verification details for each copy.\n        for copy_idx in range(VALIDATION_COPY_COUNT):\n            data[f\"verification_completion_text_{copy_idx}\"] = [\n                completion_texts[copy_idx] if completion_texts else None\n                for completion_texts in verification_completion_texts_aggregated\n            ]\n            data[f\"verification_completion_length_{copy_idx}\"] = [\n                count_tokens(completion_texts[copy_idx]) if completion_texts else None\n                for completion_texts in verification_completion_texts_aggregated\n            ]\n            data[f\"judgment_{copy_idx}\"] = [\n                judgments[copy_idx] if judgments else None\n                for judgments in judgments_aggregated\n            ]\n        data[\"judgment_count_true\"] = [judgments.count(True) for judgments in judgments_aggregated]\n        pd.DataFrame(data).to_csv(\n            f\"{str(int(time.time() - start_time)).zfill(5)}.csv\", index=False\n        )\n    \n    return best_patch\n","metadata":{"_uuid":"d5aa7692-14fa-46cb-81aa-4a05d2990df4","_cell_guid":"41d164a7-440a-4b3d-b82b-9473575c89a4","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-02-23T22:13:54.582308Z","iopub.execute_input":"2025-02-23T22:13:54.582511Z","iopub.status.idle":"2025-02-23T22:13:54.594371Z","shell.execute_reply.started":"2025-02-23T22:13:54.582494Z","shell.execute_reply":"2025-02-23T22:13:54.593785Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"import io\nfrom typing import Optional, List\n\nskip_prediction: bool = False\n\n\ndef predict(\n    problem_statement: str,\n    repo_archive: io.BytesIO,\n    pip_packages_archive: io.BytesIO,\n    env_setup_cmds_templates: List[str],\n) -> Optional[str]:\n    \"\"\"Replace this function with your inference code.\n    Args:\n        problem_statement: The text of the git issue.\n        repo_archive: A BytesIO buffer path with a .tar containing the codebase that must be patched. The gateway will make this directory available immediately before this function runs.\n    \"\"\"\n    global skip_prediction\n    if skip_prediction:\n        return None\n\n    with open(\"repo_archive.tar\", \"wb\") as f:\n        f.write(repo_archive.read())\n    repo_path: str = REPO_PATH\n    if os.path.exists(repo_path):\n        shutil.rmtree(repo_path)\n    shutil.unpack_archive(\"repo_archive.tar\", extract_dir=repo_path)\n    os.remove(\"repo_archive.tar\")\n\n    patch_string: Optional[str] = None\n    patch_string = predict_inner(\n        problem_statement=problem_statement, directory=repo_path\n    )\n    shutil.rmtree(repo_path)\n\n    if not os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n        skip_prediction = True\n\n    print(\"submitted patch_string\")\n    print(patch_string)\n\n    if patch_string is None:\n        return None\n\n    return patch_string","metadata":{"_uuid":"1fe33c0a-f4bb-4d58-a31a-8a35efed47f4","_cell_guid":"f6603ce1-36ca-4c32-b6b7-55d970a38851","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-23T22:13:54.594962Z","iopub.execute_input":"2025-02-23T22:13:54.595177Z","iopub.status.idle":"2025-02-23T22:13:54.606657Z","shell.execute_reply.started":"2025-02-23T22:13:54.595160Z","shell.execute_reply":"2025-02-23T22:13:54.606122Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"# Get predict data without server","metadata":{"_uuid":"ec5157f3-fc1a-4944-a94b-d472995e12cd","_cell_guid":"81b9a808-f41a-4715-aafc-73184d4cc947","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# import os\n# import zipfile\n\n# # !mkdir -p /kaggle/tmp/konwinski-prize-alt\n# os.makedirs(\"/kaggle/tmp/konwinski-prize-alt\", exist_ok=True)\n\n# # !unzip -q -o /kaggle/input/konwinski-prize/data.a_zip -d /kaggle/tmp/konwinski-prize-alt/ 2>/dev/null || true\n# try:\n#     with zipfile.ZipFile(\"/kaggle/input/konwinski-prize/data.a_zip\", \"r\") as zip_ref:\n#         zip_ref.extractall(\"/kaggle/tmp/konwinski-prize-alt/\")\n# except:\n#     pass","metadata":{"_uuid":"6d66b2c6-91e0-4652-b671-a2534be7e103","_cell_guid":"91157abf-a02f-47f2-b6b2-64057f65adaa","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-23T22:13:54.607297Z","iopub.execute_input":"2025-02-23T22:13:54.607495Z","iopub.status.idle":"2025-02-23T22:13:54.618239Z","shell.execute_reply.started":"2025-02-23T22:13:54.607478Z","shell.execute_reply":"2025-02-23T22:13:54.617585Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# import pandas as pd\n\n\n# def get_problem(problem_index: int) -> Tuple[str, str, io.BytesIO]:\n#     df = pd.read_parquet(\"/kaggle/tmp/konwinski-prize-alt/data/data.parquet\")\n\n#     problem_statement: str = df[\"problem_statement\"][problem_index]\n#     repo_path: str = (\n#         f\"/kaggle/tmp/konwinski-prize-alt/data/repos/repo__{df['instance_id'][problem_index]}\"\n#     )\n\n#     import shutil\n#     import tempfile\n\n#     with tempfile.TemporaryDirectory() as tmpdir:\n#         shutil.make_archive(os.path.join(tmpdir, \"a_repo\"), \"tar\", repo_path)\n#         with open(os.path.join(tmpdir, \"a_repo.tar\"), \"rb\") as f:\n#             repo_archive = io.BytesIO(f.read())\n\n#     return problem_statement, repo_path, repo_archive","metadata":{"_uuid":"46af49a7-33eb-4d09-9830-7c96f125cae4","_cell_guid":"eb711b99-4aff-48a8-8512-a00e18923f41","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-23T22:13:54.618943Z","iopub.execute_input":"2025-02-23T22:13:54.619188Z","iopub.status.idle":"2025-02-23T22:13:54.627308Z","shell.execute_reply.started":"2025-02-23T22:13:54.619170Z","shell.execute_reply":"2025-02-23T22:13:54.626749Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# demo_problem_index: int = 0\n\n# if os.getenv(\"KAGGLE_KERNEL_RUN_TYPE\") == \"Interactive\" and not os.getenv(\n#     \"KAGGLE_IS_COMPETITION_RERUN\"\n# ):\n#     problem_statement, repo_path, repo_archive = get_problem(\n#         problem_index=demo_problem_index\n#     )\n\n#     print(repo_path)\n#     print(problem_statement)\n#     print(len(list(repo_archive)))\n#     print(len(list(repo_archive)))","metadata":{"_uuid":"71445695-cf75-4bdf-bf38-e9d5fdc7c07c","_cell_guid":"61fe4186-964b-4385-8837-e2f33c44c722","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-23T22:13:54.629413Z","iopub.execute_input":"2025-02-23T22:13:54.629620Z","iopub.status.idle":"2025-02-23T22:13:54.639340Z","shell.execute_reply.started":"2025-02-23T22:13:54.629603Z","shell.execute_reply":"2025-02-23T22:13:54.638750Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# if os.getenv(\"KAGGLE_KERNEL_RUN_TYPE\") == \"Interactive\" and not os.getenv(\n#     \"KAGGLE_IS_COMPETITION_RERUN\"\n# ):\n#     skip_prediction = False\n#     problem_statement, repo_path, repo_archive = get_problem(\n#         problem_index=demo_problem_index\n#     )\n#     patch_string = predict(problem_statement, repo_archive, io.BytesIO(), [])","metadata":{"_uuid":"40eac664-ddd7-462d-b9a2-6a65353e8b5a","_cell_guid":"ce7abf42-5abe-41b4-aca7-3c7302749713","trusted":true,"collapsed":false,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2025-02-23T22:13:54.640134Z","iopub.execute_input":"2025-02-23T22:13:54.640330Z","iopub.status.idle":"2025-02-23T22:13:54.649795Z","shell.execute_reply.started":"2025-02-23T22:13:54.640314Z","shell.execute_reply":"2025-02-23T22:13:54.649182Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# if (\n#     os.getenv(\"KAGGLE_KERNEL_RUN_TYPE\") == \"Interactive\"\n#     and not os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\")\n#     and patch_string is not None\n# ):\n#     import polars as pl\n\n#     df = pl.read_parquet(\"/kaggle/tmp/konwinski-prize-alt/data/data.parquet\")\n\n#     import kaggle_evaluation.konwinski_prize_gateway\n\n#     k_prize_gateway = kaggle_evaluation.konwinski_prize_gateway.KPrizeGateway()\n#     k_prize_gateway.unpack_data_paths()\n\n#     results = k_prize_gateway._evaluate_instance(\n#         instance=df.row(demo_problem_index, named=True),\n#         patch=patch_string,\n#     )\n\n#     from collections import Counter\n#     print(\n#         demo_problem_index, Counter(result.unit_test_outcome for result in results[1:])\n#     )","metadata":{"_uuid":"c00544c0-d176-4d48-a025-6cbac48576ec","_cell_guid":"11968fc9-c39e-4088-b953-74ea01632ad3","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-23T22:13:54.650483Z","iopub.execute_input":"2025-02-23T22:13:54.650693Z","iopub.status.idle":"2025-02-23T22:13:54.659637Z","shell.execute_reply.started":"2025-02-23T22:13:54.650676Z","shell.execute_reply":"2025-02-23T22:13:54.659075Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# if (\n#     os.getenv(\"KAGGLE_KERNEL_RUN_TYPE\") == \"Interactive\"\n#     and not os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\")\n#     and patch_string is not None\n# ):\n#     from kaggle_evaluation.konwinski_prize_gateway import UnitTestOutcome\n\n#     for result in results[1:]:\n#         if result.unit_test_outcome != UnitTestOutcome.PASSED:\n#             print(result.test_name)\n#             print(result.fail_description)\n            ","metadata":{"_uuid":"db5f7042-adf4-42c0-918e-cdb9031cd545","_cell_guid":"74a18506-0c71-4c43-828e-ac63c9f066a0","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-23T22:13:54.660284Z","iopub.execute_input":"2025-02-23T22:13:54.660488Z","iopub.status.idle":"2025-02-23T22:13:54.672285Z","shell.execute_reply.started":"2025-02-23T22:13:54.660472Z","shell.execute_reply":"2025-02-23T22:13:54.671734Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"When your notebook is run on the hidden test set, inference_server.serve must be called within 15 minutes of the notebook starting or the gateway will throw an error. If you need more than 15 minutes to load your model you can do so during the very first predict call, which does not have the usual 30 minute response deadline.","metadata":{"_uuid":"3547dec1-18d2-49b0-8c08-ac09d8193723","_cell_guid":"3c0427bb-5570-4314-83bd-bc17d09fe3c7","trusted":true,"collapsed":false,"papermill":{"duration":0.001889,"end_time":"2024-12-11T03:22:08.856283","exception":false,"start_time":"2024-12-11T03:22:08.854394","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"# Evaluation with inference server","metadata":{"_uuid":"c6870d06-100c-4160-9cf5-311910a84007","_cell_guid":"ac0ac0e2-d8c1-4e8b-8415-ae41b202d87d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"skip_prediction = False","metadata":{"_uuid":"6a96bb2f-85cc-4b37-b044-1ecbf8422efd","_cell_guid":"697c6852-96c5-4c3f-85af-64b340b1c65c","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-23T22:13:54.672867Z","iopub.execute_input":"2025-02-23T22:13:54.673088Z","iopub.status.idle":"2025-02-23T22:13:54.683373Z","shell.execute_reply.started":"2025-02-23T22:13:54.673070Z","shell.execute_reply":"2025-02-23T22:13:54.682793Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"inference_server = (\n    kaggle_evaluation.konwinski_prize_inference_server.KPrizeInferenceServer(\n        get_number_of_instances, predict\n    )\n)\n\nif os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n    inference_server.serve()\nelse:\n    inference_server.run_local_gateway(\n        data_paths=(\n            \"/kaggle/input/konwinski-prize/\",  # Path to the entire competition dataset\n            \"/kaggle/tmp/konwinski-prize/\",  # Path to a scratch directory for unpacking data.a_zip.\n        ),  # type: ignore\n        \n        use_concurrency=True,\n    )","metadata":{"_uuid":"e03ade0d-f48f-4682-888e-bd8d258767f3","_cell_guid":"bbbf2217-ac29-4d86-bffd-afe648ae1167","trusted":true,"collapsed":false,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2025-02-23T22:13:54.683952Z","iopub.execute_input":"2025-02-23T22:13:54.684173Z","iopub.status.idle":"2025-02-23T22:14:05.776363Z","shell.execute_reply.started":"2025-02-23T22:13:54.684155Z","shell.execute_reply":"2025-02-23T22:14:05.775360Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"Existing uv installation found. Skipping uv installation.\nInstalling Python 3.11...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mGatewayRuntimeError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-bc732f36f9dc>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0minference_server\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     inference_server.run_local_gateway(\n\u001b[0m\u001b[1;32m     11\u001b[0m         data_paths=(\n\u001b[1;32m     12\u001b[0m             \u001b[0;34m\"/kaggle/input/konwinski-prize/\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Path to the entire competition dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/input/konwinski-prize/kaggle_evaluation/core/templates.py\u001b[0m in \u001b[0;36mrun_local_gateway\u001b[0;34m(self, data_paths, file_share_dir, *args, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/input/konwinski-prize/kaggle_evaluation/core/templates.py\u001b[0m in \u001b[0;36mrun_local_gateway\u001b[0;34m(self, data_paths, file_share_dir, *args, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_gateway_for_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_share_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/input/konwinski-prize/kaggle_evaluation/core/templates.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;31m# For local testing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/input/konwinski-prize/kaggle_evaluation/core/templates.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_data_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_submission\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mkaggle_evaluation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_gateway\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGatewayRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgre\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/input/konwinski-prize/kaggle_evaluation/konwinski_prize_gateway.py\u001b[0m in \u001b[0;36mget_all_predictions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0menv_setup_cmd_templates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_kprize_env_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_env_setup_cmds_templates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_config_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m             patch = self.predict(\n\u001b[0m\u001b[1;32m    442\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"problem_statement\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m                 \u001b[0mrepo_buffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/input/konwinski-prize/kaggle_evaluation/core/templates.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'predict'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_server_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'predict'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_response_timeout_seconds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_seconds\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/input/konwinski-prize/kaggle_evaluation/core/base_gateway.py\u001b[0m in \u001b[0;36mhandle_server_error\u001b[0;34m(self, exception, endpoint)\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0mmessage_match\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\"Exception calling application: (.*)\"'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexception_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIGNORECASE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmessage_match\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmessage_match\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mexception_str\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mGatewayRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGatewayRuntimeErrorType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSERVER_RAISED_EXCEPTION\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_channel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_InactiveRpcError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mGatewayRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGatewayRuntimeErrorType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSERVER_CONNECTION_FAILED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexception_str\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mGatewayRuntimeError\u001b[0m: (<GatewayRuntimeErrorType.SERVER_RAISED_EXCEPTION: 3>, 'Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating')"],"ename":"GatewayRuntimeError","evalue":"(<GatewayRuntimeErrorType.SERVER_RAISED_EXCEPTION: 3>, 'Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating')","output_type":"error"}],"execution_count":23},{"cell_type":"code","source":"","metadata":{"_uuid":"86683ecc-947a-444b-b073-352bf009e91c","_cell_guid":"b73f8a85-f747-4a0d-9598-6bf418752bc6","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}