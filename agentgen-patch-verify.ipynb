{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaL4","dataSources":[{"sourceId":84795,"databundleVersionId":10934030,"sourceType":"competition"},{"sourceId":221096520,"sourceType":"kernelVersion"},{"sourceId":236932,"sourceType":"modelInstanceVersion","modelInstanceId":202348,"modelId":224071}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n\n# https://www.kaggle.com/competitions/ai-mathematical-olympiad-progress-prize-2/discussion/560682#3113134\nos.environ[\"TRITON_PTXAS_PATH\"] = \"/usr/local/cuda/bin/ptxas\"","metadata":{"_uuid":"38b86f00-408f-4577-b91e-890da62fd434","_cell_guid":"eec2b282-90f3-4965-a943-54cffbeb7a5b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-02-22T10:29:01.513422Z","iopub.execute_input":"2025-02-22T10:29:01.513620Z","iopub.status.idle":"2025-02-22T10:29:01.516474Z","shell.execute_reply.started":"2025-02-22T10:29:01.513592Z","shell.execute_reply":"2025-02-22T10:29:01.515924Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import io\nimport time\nimport shutil\n\nimport pandas as pd\nimport polars as pl\n\nimport kaggle_evaluation.konwinski_prize_inference_server\nfrom typing import List, Tuple, Dict, Optional\n\nstart_time = time.time()","metadata":{"_uuid":"e9ab5832-846d-4fb5-b21b-532f5962301d","_cell_guid":"2512bd4d-9caf-4c50-9706-0377f47811ae","trusted":true,"collapsed":false,"_kg_hide-output":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-02-22T10:29:01.517083Z","iopub.execute_input":"2025-02-22T10:29:01.517269Z","iopub.status.idle":"2025-02-22T10:29:13.457141Z","shell.execute_reply.started":"2025-02-22T10:29:01.517252Z","shell.execute_reply":"2025-02-22T10:29:13.456432Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"The evaluation API requires that you set up a server which will respond to inference requests. We have already defined the server; you just need write the predict function. When we evaluate your submission on the hidden test set the client defined in `konwinski_prize_gateway` will run in a different container with direct access to the hidden test set and hand off the data.\n#\nYour code will always have access to the published copies of the files.","metadata":{"_uuid":"acfccb6c-54ff-466d-b3ca-dc9d610e1562","_cell_guid":"9b41a507-839f-49ba-bed3-7ec21f5bd402","trusted":true,"collapsed":false,"papermill":{"duration":0.002032,"end_time":"2024-12-11T03:22:08.823897","exception":false,"start_time":"2024-12-11T03:22:08.821865","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"instance_count: Optional[int] = None\n\n\ndef get_number_of_instances(num_instances: int) -> None:\n    \"\"\"The very first message from the gateway will be the total number of instances to be served.\n    You don't need to edit this function.\n    \"\"\"\n    global instance_count\n    instance_count = num_instances","metadata":{"_uuid":"f251dce7-62ae-4856-9c05-f5ffe2938eaf","_cell_guid":"7fce3650-d66d-4189-ac61-51f4e07ac289","trusted":true,"collapsed":false,"papermill":{"duration":0.011949,"end_time":"2024-12-11T03:22:08.838279","exception":false,"start_time":"2024-12-11T03:22:08.82633","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-02-22T10:29:13.457797Z","iopub.execute_input":"2025-02-22T10:29:13.458436Z","iopub.status.idle":"2025-02-22T10:29:13.461735Z","shell.execute_reply.started":"2025-02-22T10:29:13.458412Z","shell.execute_reply":"2025-02-22T10:29:13.461125Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Initialize LLM","metadata":{"_uuid":"370e67e3-a871-42b0-b0a3-ab7bbcf87dcc","_cell_guid":"e00a129e-16b7-408e-a131-75c4c476b300","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"from vllm import LLM, SamplingParams, RequestOutput\nimport warnings\n\nwarnings.simplefilter(\"ignore\")\n\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nif os.getenv(\"KAGGLE_KERNEL_RUN_TYPE\") or os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n    llm_model_pth: str = (\n        \"/kaggle/input/m/shelterw/deepseek-r1/transformers/deepseek-r1-distill-qwen-32b-awq/1\"\n    )\nelse:\n    llm_model_pth: str = \"/root/volume/KirillR/QwQ-32B-Preview-AWQ\"\n\nBATCH_SIZE: int = 6\nVALIDATION_COPY_COUNT: int = 1\nMAX_TOKENS: int = 4096\n\nMAX_NUM_SEQS: int = 6\nMAX_MODEL_LEN: int = 32_768\n\nllm: LLM = LLM(\n    llm_model_pth,\n    max_num_seqs=MAX_NUM_SEQS,  # Maximum number of sequences per iteration. Default is 256\n    max_model_len=MAX_MODEL_LEN,  # Model context length\n    trust_remote_code=True,  # Trust remote code (e.g., from HuggingFace) when downloading the model and tokenizer\n    tensor_parallel_size=4,  # The number of GPUs to use for distributed execution with tensor parallelism\n    gpu_memory_utilization=0.95,  # The ratio (between 0 and 1) of GPU memory to reserve for the model\n    seed=2024,\n)","metadata":{"_uuid":"eca28397-2a40-4265-b36e-dfa9d80eb2b1","_cell_guid":"c863b090-e04a-441e-9add-bddae0863ef9","trusted":true,"collapsed":false,"_kg_hide-output":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-02-22T10:29:13.462386Z","iopub.execute_input":"2025-02-22T10:29:13.462580Z","iopub.status.idle":"2025-02-22T10:34:03.897102Z","shell.execute_reply.started":"2025-02-22T10:29:13.462562Z","shell.execute_reply":"2025-02-22T10:34:03.896301Z"}},"outputs":[{"name":"stdout","text":"INFO 02-22 10:30:02 __init__.py:183] Automatically detected platform cuda.\nINFO 02-22 10:30:34 config.py:526] This model supports multiple tasks: {'reward', 'embed', 'score', 'classify', 'generate'}. Defaulting to 'generate'.\nINFO 02-22 10:30:38 awq_marlin.py:109] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\nINFO 02-22 10:30:38 config.py:1383] Defaulting to use mp for distributed inference\nWARNING 02-22 10:30:38 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\nINFO 02-22 10:30:38 llm_engine.py:232] Initializing a V0 LLM engine (v0.7.1) with config: model='/kaggle/input/m/shelterw/deepseek-r1/transformers/deepseek-r1-distill-qwen-32b-awq/1', speculative_config=None, tokenizer='/kaggle/input/m/shelterw/deepseek-r1/transformers/deepseek-r1-distill-qwen-32b-awq/1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=2024, served_model_name=/kaggle/input/m/shelterw/deepseek-r1/transformers/deepseek-r1-distill-qwen-32b-awq/1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[8,4,2,1],\"max_capture_size\":8}, use_cached_outputs=False, \nWARNING 02-22 10:30:38 multiproc_worker_utils.py:298] Reducing Torch parallelism from 24 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\nINFO 02-22 10:30:38 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n\u001b[1;36m(VllmWorkerProcess pid=535)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=536)\u001b[0;0m INFO 02-22 10:30:38 multiproc_worker_utils.py:227] Worker ready; awaiting tasks\nINFO 02-22 10:30:38 multiproc_worker_utils.py:227] Worker ready; awaiting tasks\nINFO 02-22 10:30:38 multiproc_worker_utils.py:227] Worker ready; awaiting tasks\n\u001b[1;36m(VllmWorkerProcess pid=541)\u001b[0;0m WARNING 02-22 10:30:39 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n\u001b[1;36m(VllmWorkerProcess pid=536)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=541)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=535)\u001b[0;0m WARNING 02-22 10:30:39 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\nWARNING 02-22 10:30:39 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\nWARNING 02-22 10:30:39 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n\u001b[1;36m(VllmWorkerProcess pid=535)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=536)\u001b[0;0m INFO 02-22 10:30:39 cuda.py:235] Using Flash Attention backend.\n\u001b[1;36m(VllmWorkerProcess pid=541)\u001b[0;0m INFO 02-22 10:30:39 cuda.py:235] Using Flash Attention backend.\nINFO 02-22 10:30:39 cuda.py:235] Using Flash Attention backend.\nINFO 02-22 10:30:39 cuda.py:235] Using Flash Attention backend.\nINFO 02-22 10:30:50 utils.py:938] Found nccl from library libnccl.so.2\nINFO 02-22 10:30:50 pynccl.py:67] vLLM is using nccl==2.21.5\n\u001b[1;36m(VllmWorkerProcess pid=536)\u001b[0;0m INFO 02-22 10:30:50 utils.py:938] Found nccl from library libnccl.so.2\n\u001b[1;36m(VllmWorkerProcess pid=536)\u001b[0;0m INFO 02-22 10:30:50 pynccl.py:67] vLLM is using nccl==2.21.5\n\u001b[1;36m(VllmWorkerProcess pid=535)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=541)\u001b[0;0m INFO 02-22 10:30:50 utils.py:938] Found nccl from library libnccl.so.2\nINFO 02-22 10:30:50 utils.py:938] Found nccl from library libnccl.so.2\n\u001b[1;36m(VllmWorkerProcess pid=535)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=541)\u001b[0;0m INFO 02-22 10:30:50 pynccl.py:67] vLLM is using nccl==2.21.5\nINFO 02-22 10:30:50 pynccl.py:67] vLLM is using nccl==2.21.5\nWARNING 02-22 10:30:51 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n\u001b[1;36m(VllmWorkerProcess pid=541)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=536)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=535)\u001b[0;0m WARNING 02-22 10:30:51 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\nWARNING 02-22 10:30:51 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\nWARNING 02-22 10:30:51 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\nINFO 02-22 10:30:51 shm_broadcast.py:256] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_80a1d937'), local_subscribe_port=48579, remote_subscribe_port=None)\nINFO 02-22 10:30:51 model_runner.py:1111] Starting to load model /kaggle/input/m/shelterw/deepseek-r1/transformers/deepseek-r1-distill-qwen-32b-awq/1...\n\u001b[1;36m(VllmWorkerProcess pid=535)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=541)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=536)\u001b[0;0m INFO 02-22 10:30:51 model_runner.py:1111] Starting to load model /kaggle/input/m/shelterw/deepseek-r1/transformers/deepseek-r1-distill-qwen-32b-awq/1...\nINFO 02-22 10:30:51 model_runner.py:1111] Starting to load model /kaggle/input/m/shelterw/deepseek-r1/transformers/deepseek-r1-distill-qwen-32b-awq/1...\nINFO 02-22 10:30:51 model_runner.py:1111] Starting to load model /kaggle/input/m/shelterw/deepseek-r1/transformers/deepseek-r1-distill-qwen-32b-awq/1...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"692b5e55cf8c4e9d8ef84545ae2224f6"}},"metadata":{}},{"name":"stdout","text":"\u001b[1;36m(VllmWorkerProcess pid=541)\u001b[0;0m INFO 02-22 10:33:15 model_runner.py:1116] Loading model weights took 4.5673 GB\nINFO 02-22 10:33:16 model_runner.py:1116] Loading model weights took 4.5673 GB\n\u001b[1;36m(VllmWorkerProcess pid=535)\u001b[0;0m INFO 02-22 10:33:16 model_runner.py:1116] Loading model weights took 4.5673 GB\n\u001b[1;36m(VllmWorkerProcess pid=536)\u001b[0;0m INFO 02-22 10:33:16 model_runner.py:1116] Loading model weights took 4.5673 GB\n\u001b[1;36m(VllmWorkerProcess pid=541)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=536)\u001b[0;0m WARNING 02-22 10:33:54 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\nWARNING 02-22 10:33:54 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n\u001b[1;36m(VllmWorkerProcess pid=541)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=536)\u001b[0;0m WARNING 02-22 10:33:54 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n\u001b[1;36m(VllmWorkerProcess pid=535)\u001b[0;0m WARNING 02-22 10:33:54 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\nWARNING 02-22 10:33:54 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n\u001b[1;36m(VllmWorkerProcess pid=541)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=536)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=535)\u001b[0;0m INFO 02-22 10:33:54 worker.py:266] Memory profiling takes 38.28 seconds\nINFO 02-22 10:33:54 worker.py:266] Memory profiling takes 38.28 seconds\nWARNING 02-22 10:33:54 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n\u001b[1;36m(VllmWorkerProcess pid=541)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=536)\u001b[0;0m INFO 02-22 10:33:54 worker.py:266] the current vLLM instance can use total_gpu_memory (22.28GiB) x gpu_memory_utilization (0.95) = 21.16GiB\n\u001b[1;36m(VllmWorkerProcess pid=535)\u001b[0;0m INFO 02-22 10:33:54 worker.py:266] the current vLLM instance can use total_gpu_memory (22.28GiB) x gpu_memory_utilization (0.95) = 21.16GiB\n\u001b[1;36m(VllmWorkerProcess pid=541)\u001b[0;0m INFO 02-22 10:33:54 worker.py:266] Memory profiling takes 38.29 seconds\n\u001b[1;36m(VllmWorkerProcess pid=536)\u001b[0;0m INFO 02-22 10:33:54 worker.py:266] model weights take 4.57GiB; non_torch_memory takes 0.17GiB; PyTorch activation peak memory takes 2.96GiB; the rest of the memory reserved for KV Cache is 13.47GiB.\n\u001b[1;36m(VllmWorkerProcess pid=535)\u001b[0;0m INFO 02-22 10:33:54 worker.py:266] model weights take 4.57GiB; non_torch_memory takes 0.17GiB; PyTorch activation peak memory takes 2.96GiB; the rest of the memory reserved for KV Cache is 13.47GiB.\nINFO 02-22 10:33:54 worker.py:266] the current vLLM instance can use total_gpu_memory (22.28GiB) x gpu_memory_utilization (0.95) = 21.16GiB\n\u001b[1;36m(VllmWorkerProcess pid=535)\u001b[0;0m INFO 02-22 10:33:54 worker.py:266] model weights take 4.57GiB; non_torch_memory takes 0.17GiB; PyTorch activation peak memory takes 2.96GiB; the rest of the memory reserved for KV Cache is 13.47GiB.\nWARNING 02-22 10:33:55 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\nWARNING 02-22 10:33:55 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\nINFO 02-22 10:33:55 worker.py:266] Memory profiling takes 38.62 seconds\nINFO 02-22 10:33:55 worker.py:266] the current vLLM instance can use total_gpu_memory (22.28GiB) x gpu_memory_utilization (0.95) = 21.16GiB\nINFO 02-22 10:33:55 worker.py:266] model weights take 4.57GiB; non_torch_memory takes 0.17GiB; PyTorch activation peak memory takes 2.96GiB; the rest of the memory reserved for KV Cache is 13.47GiB.\nINFO 02-22 10:33:55 executor_base.py:108] # CUDA blocks: 13794, # CPU blocks: 4096\nINFO 02-22 10:33:55 executor_base.py:113] Maximum concurrency for 32768 tokens per request: 6.74x\nWARNING 02-22 10:33:55 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\nWARNING 02-22 10:33:55 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n\u001b[1;36m(VllmWorkerProcess pid=541)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=536)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=535)\u001b[0;0m WARNING 02-22 10:33:55 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\nWARNING 02-22 10:33:55 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\nWARNING 02-22 10:33:55 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n\u001b[1;36m(VllmWorkerProcess pid=541)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=536)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=535)\u001b[0;0m WARNING 02-22 10:33:55 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\nWARNING 02-22 10:33:55 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\nWARNING 02-22 10:33:55 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\nINFO 02-22 10:33:58 model_runner.py:1435] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n\u001b[1;36m(VllmWorkerProcess pid=536)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=541)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=535)\u001b[0;0m INFO 02-22 10:33:58 model_runner.py:1435] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\nINFO 02-22 10:33:58 model_runner.py:1435] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\nINFO 02-22 10:33:58 model_runner.py:1435] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n","output_type":"stream"},{"name":"stderr","text":"Capturing CUDA graph shapes: 100%|██████████| 4/4 [00:04<00:00,  1.23s/it]","output_type":"stream"},{"name":"stdout","text":"INFO 02-22 10:34:03 model_runner.py:1563] Graph capturing finished in 5 secs, took 0.08 GiB\n\u001b[1;36m(VllmWorkerProcess pid=536)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=541)\u001b[0;0m INFO 02-22 10:34:03 model_runner.py:1563] Graph capturing finished in 5 secs, took 0.08 GiB\nINFO 02-22 10:34:03 model_runner.py:1563] Graph capturing finished in 5 secs, took 0.08 GiB\n\u001b[1;36m(VllmWorkerProcess pid=535)\u001b[0;0m INFO 02-22 10:34:03 model_runner.py:1563] Graph capturing finished in 5 secs, took 0.08 GiB\nINFO 02-22 10:34:03 llm_engine.py:429] init engine (profile, create kv cache, warmup model) took 47.44 seconds\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"tokenizer = llm.get_tokenizer()\n\n\ndef count_tokens(text: str) -> int:\n    return len(tokenizer.encode(text))","metadata":{"_uuid":"5a982fe4-e4e2-466d-b5bf-4839ec8804c8","_cell_guid":"99e40ae3-6474-4b05-a969-0b48de303ba6","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-02-22T10:34:03.900705Z","iopub.execute_input":"2025-02-22T10:34:03.900934Z","iopub.status.idle":"2025-02-22T10:34:03.904280Z","shell.execute_reply.started":"2025-02-22T10:34:03.900915Z","shell.execute_reply":"2025-02-22T10:34:03.903688Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# Helper functions","metadata":{"_uuid":"30f587b9-7b4f-4812-82f7-d87787c58d1c","_cell_guid":"3a54e069-fe31-4873-9d25-a02c38576c59","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import os\n\n\ndef stringify_directory(directory: str) -> str:\n    full_paths: List[str] = []\n\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            full_path: str = os.path.join(root, file)\n            full_paths.append(full_path)\n    return \"\\n\".join(full_paths)","metadata":{"_uuid":"43546fee-8473-4f0d-80ec-46a86369426b","_cell_guid":"44d40acd-8428-4813-ba5d-62f625309dd4","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-02-22T10:34:03.904906Z","iopub.execute_input":"2025-02-22T10:34:03.905104Z","iopub.status.idle":"2025-02-22T10:34:03.919689Z","shell.execute_reply.started":"2025-02-22T10:34:03.905086Z","shell.execute_reply":"2025-02-22T10:34:03.919064Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import re\n\n\ndef extract_file_query(xml_content: str) -> Dict[str, List[str]]:\n    import xml.etree.ElementTree as ET\n\n    # Prepare a data structure to collect results\n    parsed_data: Dict[str, List[str]] = {}\n    pattern: str = r\"<root>(.*?)</root>\"\n    matches: List[str] = re.findall(pattern, xml_content, re.DOTALL)\n\n    for match in matches:\n        try:\n            # Parse the XML\n            root = ET.fromstring(\"<root>\" + match + \"</root>\")\n\n            # Find all <entry> elements\n            for entry in root.findall(\"entry\"):\n                # Extract the <filepath> text\n                filepath = entry.find(\"filepath\")\n                filepath_text: Optional[str] = (\n                    filepath.text.strip()\n                    if filepath is not None and filepath.text is not None\n                    else None\n                )\n\n                # Locate <strings_to_search> container\n                strings_container = entry.find(\"strings_to_search\")\n\n                # Gather each <string_to_search> text\n                search_strings: List[str] = []\n                if strings_container is not None:\n                    for s in strings_container.findall(\"string_to_search\"):\n                        if s.text is not None:\n                            search_strings.append(s.text.strip())\n\n                # Store in a dictionary: { filepath: [search_strings...] }\n                parsed_data[filepath_text] = search_strings  # type: ignore\n        except:\n            print(\"Error parsing output\")\n            print(xml_content)\n            return {}\n\n    return parsed_data","metadata":{"_uuid":"47b13790-f9e8-47cd-8b78-c93567e04b56","_cell_guid":"50b46124-4395-4783-b523-f11f4b6228bf","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-02-22T10:34:03.920335Z","iopub.execute_input":"2025-02-22T10:34:03.920540Z","iopub.status.idle":"2025-02-22T10:34:03.931716Z","shell.execute_reply.started":"2025-02-22T10:34:03.920523Z","shell.execute_reply":"2025-02-22T10:34:03.931238Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"reading_prompt: str = (\n    \"\"\"\nYou will be implementing a git diff patch to solve an issue with the code repository.\nYou will first need to select files in the file directory.\n\nThis is the problem statement.\n\n{problem_statement}\n\nThis is the file directory\n\n<directory>\n{directory_string}\n</directory>\n\nWhich files should be inspected so that we can solve the problem?\nWhen we inspect each file, what strings should be searched?\n\nReturn the strings to search in this format\n\n(explanation)\n\n<root>\n    <entry>\n        <filepath>filepath</filepath>  \n        <strings_to_search>\n            <string_to_search>string_to_search</string_to_search>\n            ...\n            <string_to_search>string_to_search</string_to_search>\n        </strings_to_search>\n    </entry>\n    <entry>\n        <filepath>filepath</filepath>\n        <strings_to_search>\n            <string_to_search>string_to_search</string_to_search>\n            ...\n            <string_to_search>string_to_search</string_to_search>\n        </strings_to_search>\n    </entry>\n    ...\n</root>\n...\n\nNotes:\n- Make sure to encode each entry between <root> and </root>\n- Return the FULL filepath - exactly as specified in <directory> and </directory>\n    - Example: <filepath>repo/path/to/directory/file.py</filepath>\n- If you are searching for a word instead of a substring, maybe add spaces or brackets before and after the string\n    - For example, if you are searching for uses of the function `calculate`, use ` calculate(` as the search string instead of `calculate`\n- Prefer searching longer strings\n    - Avoid searching for strings that might appear in many parts of the codebase\n- Search the test files as well to understand the feature behavior\n    - Also search for the relevant function calls in the test files\n\"\"\".strip()\n)\n\n\ndef get_selection_query(\n    directory_string: str, problem_statement: str\n) -> Tuple[List[str], List[Dict[str, List[str]]]]:\n    sampling_params: SamplingParams = SamplingParams(\n        temperature=0.6,  # randomness of the sampling\n        min_p=0.01,\n        skip_special_tokens=True,  # Whether to skip special tokens in the output\n        max_tokens=MAX_TOKENS,\n    )\n\n    list_of_messages: List[List[Dict[str, str]]] = [\n        [\n            {\n                \"role\": \"user\",\n                \"content\": reading_prompt.format(\n                    problem_statement=problem_statement[:20_000],\n                    directory_string=directory_string[:30_000],\n                ),\n            },\n        ]\n        for _ in range(BATCH_SIZE)\n    ]\n\n    prompt_texts: List[str] = [\n        (\n            tokenizer.apply_chat_template(\n                conversation=messages, tokenize=False, add_generation_prompt=True\n            )  # type: ignore\n        )\n        + \"<think>\\n\"\n        for messages in list_of_messages\n    ]\n    # print(prompt_texts)\n\n    print(\"get_selection_query\", [count_tokens(text) for text in prompt_texts])\n    request_outputs: list[RequestOutput] = llm.generate(\n        prompt_texts, sampling_params=sampling_params\n    )\n    if not request_outputs:\n        return [], []\n    response_texts: List[str] = [\n        request_output.outputs[0].text for request_output in request_outputs\n    ]\n    print(\"get_selection_query\", [count_tokens(text) for text in response_texts])\n\n    completion_texts = [\n        prompt_text + response_text\n        for prompt_text, response_text in zip(prompt_texts, response_texts)\n    ]\n    file_queries: List[Dict[str, List[str]]] = [\n        extract_file_query(response_text) for response_text in response_texts\n    ]\n    return completion_texts, file_queries","metadata":{"_uuid":"b6264935-32cf-49b5-b1da-aece43dbf184","_cell_guid":"a636bf5a-ec3a-4113-8504-bc95550036ef","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-02-22T10:34:03.932280Z","iopub.execute_input":"2025-02-22T10:34:03.932473Z","iopub.status.idle":"2025-02-22T10:34:03.942345Z","shell.execute_reply.started":"2025-02-22T10:34:03.932456Z","shell.execute_reply":"2025-02-22T10:34:03.941804Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"REPO_PATH: str = \"repo\"\n\n\ndef fetch_file_contents(\n    files_to_search: Dict[str, List[str]], context_lines: int = 12, max_gap: int = 0\n) -> str:\n    from io import StringIO\n    from typing import Tuple\n\n    def find_lines_in_files_with_context(\n        search_map: Dict[str, List[str]], context_lines: int = context_lines\n    ) -> List[List[List[Tuple[int, str]]]]:\n        \"\"\"\n        Given a dictionary mapping file paths to a list of search terms,\n        open each file and gather *snippets* of lines that contain any\n        of those search terms, including 'context_lines' before and after.\n\n        Returns a list of lists:\n        [\n          [  # For file1\n             [ (line_number, text), (line_number, text), ... ],\n             [ ... ],\n          ],\n          [  # For file2\n             ...\n          ],\n          ...\n        ]\n        \"\"\"\n        all_matches_per_file: List[List[List[Tuple[int, str]]]] = []\n\n        for path, terms in search_map.items():\n            if not os.path.isfile(path):\n                # If the file is not found, record an empty list\n                all_matches_per_file.append([])\n                continue\n\n            with open(path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n                lines = f.readlines()\n\n            file_snippets: List[List[Tuple[int, str]]] = []\n            num_lines: int = len(lines)\n\n            for i, line in enumerate(lines, start=1):\n                if any(t in line for t in terms):\n                    start_idx: int = max(1, i - context_lines)\n                    end_idx: int = min(num_lines, i + context_lines)\n                    snippet: List[Tuple[int, str]] = []\n                    for snippet_no in range(start_idx, end_idx + 1):\n                        text_content: str = lines[snippet_no - 1].rstrip(\"\\n\")\n                        snippet.append((snippet_no, text_content))\n                    file_snippets.append(snippet)\n\n            all_matches_per_file.append(file_snippets)\n\n        return all_matches_per_file\n\n    # ---------------------------------------------------------\n    # 3. MERGE OVERLAPPING/ADJACENT SNIPPETS\n    # ---------------------------------------------------------\n\n    def merge_file_snippets(\n        file_snippets: List[List[Tuple[int, str]]], gap: int = 0\n    ) -> List[List[Tuple[int, str]]]:\n        \"\"\"\n        Merge overlapping or nearly adjacent snippets in a single file’s snippet list.\n        \"\"\"\n        intervals: List[Tuple[int, int, List[Tuple[int, str]]]] = []\n        for snippet in file_snippets:\n            if snippet:\n                start_line: int = snippet[0][0]\n                end_line: int = snippet[-1][0]\n                intervals.append((start_line, end_line, snippet))\n\n        intervals.sort(key=lambda x: x[0])  # sort by start line\n\n        merged: List[Tuple[int, int, List[Tuple[int, str]]]] = []\n        for start, end, snippet in intervals:\n            if not merged:\n                merged.append((start, end, snippet))\n                continue\n\n            prev_start, prev_end, prev_snippet = merged[-1]\n            if start <= prev_end + gap:\n                new_end: int = max(end, prev_end)\n                combined_dict: Dict[int, str] = {}\n                for ln, txt in prev_snippet:\n                    combined_dict[ln] = txt\n                for ln, txt in snippet:\n                    combined_dict[ln] = txt\n                merged_snippet: List[Tuple[int, str]] = [\n                    (ln, combined_dict[ln]) for ln in sorted(combined_dict)\n                ]\n                merged[-1] = (prev_start, new_end, merged_snippet)\n            else:\n                merged.append((start, end, snippet))\n\n        # Extract just the merged snippet portion\n        return [x[2] for x in merged]\n\n    def merge_all_snippets(\n        all_files_snips: List[List[List[Tuple[int, str]]]], gap: int = 0\n    ) -> List[List[List[Tuple[int, str]]]]:\n        \"\"\"\n        Merge snippet blocks within each file.\n        all_files_snips is a list-of-lists:\n          [\n            [ snippetA, snippetB, ... ],  # file 1\n            [ snippetC, snippetD, ... ],  # file 2\n          ]\n        \"\"\"\n        merged: List[List[List[Tuple[int, str]]]] = []\n        for snips in all_files_snips:\n            merged.append(merge_file_snippets(snips, gap=gap))\n        return merged\n\n    # ---------------------------------------------------------\n    # 4. RUN LOGIC: generate files, search, merge, and BUILD A STRING\n    # ---------------------------------------------------------\n\n    has_any_matches: bool = False\n\n    # 1) Gather snippets around each match\n    context_snippets: List[List[List[Tuple[int, str]]]] = (\n        find_lines_in_files_with_context(files_to_search, context_lines=context_lines)\n    )\n\n    # 2) Merge overlapping snippets\n    merged_snips: List[List[List[Tuple[int, str]]]] = merge_all_snippets(\n        context_snippets, gap=max_gap\n    )\n\n    # 3) Build a string (instead of printing)\n    output = StringIO()\n\n    # Header\n    output.write(\"Sample files created successfully.\\n\\n\")\n    output.write(\"Search Results (by file, merging any overlapping context):\\n\\n\")\n\n    # For each file\n    for (filepath, terms), snippet_list in zip(files_to_search.items(), merged_snips):\n        output.write(f\"[file name]: {filepath[len(REPO_PATH) + 1:]}\\n\")\n        terms_searched_as_str = \"\\n\".join(terms)\n        output.write(f\"[terms searched]:\\n{terms_searched_as_str}\\n\")\n        output.write(\"[file content begin]\\n\")\n        if not snippet_list:\n            output.write(\"  No matches found.\\n\")\n        else:\n            has_any_matches = True\n            for snippet_idx, snippet in enumerate(snippet_list, start=1):\n                snippet_start: int = snippet[0][0]\n                snippet_end: int = snippet[-1][0]\n                output.write(\n                    f\"\\nMatch #{snippet_idx}, lines {snippet_start} to {snippet_end}:\\n\"\n                )\n                for line_no, text in snippet:\n                    output.write(f\"  {line_no:3d} | {text}\\n\")\n                output.write(\"\\n\")\n        output.write(\"[file content end]\\n\\n\")\n\n    file_content_string: str = output.getvalue()\n\n    if has_any_matches:\n        return file_content_string\n    return \"\"","metadata":{"_uuid":"f8ae44f3-2595-4c78-b58a-6551c0fec3a8","_cell_guid":"2e6aebd6-10e8-4769-98dc-4e89aaf35b41","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-02-22T10:34:03.942914Z","iopub.execute_input":"2025-02-22T10:34:03.943107Z","iopub.status.idle":"2025-02-22T10:34:03.958338Z","shell.execute_reply.started":"2025-02-22T10:34:03.943091Z","shell.execute_reply":"2025-02-22T10:34:03.957755Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import re\n\ndef extract_patch_string(text: str) -> Optional[str]:\n    pattern: str = r\"\\n```diff\\n(.*?)\\n```\"\n    matches: List[str] = re.findall(pattern, text, re.DOTALL)\n    if not matches:\n        return None\n    return matches[-1] + \"\\n\"","metadata":{"_uuid":"5dcae870-2699-4110-956b-121c8687bfa9","_cell_guid":"a2564919-7162-4711-a5c6-92077938254f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-02-22T10:34:03.958914Z","iopub.execute_input":"2025-02-22T10:34:03.959106Z","iopub.status.idle":"2025-02-22T10:34:03.971596Z","shell.execute_reply.started":"2025-02-22T10:34:03.959090Z","shell.execute_reply":"2025-02-22T10:34:03.971007Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"patching_prompt: str = (\n    \"\"\"\nYou will be implementing a git diff patch to solve an issue with the code repository.\nThis is the problem statement.\n\n{problem_statement}\n\nThese are the files that is thought to be relevant\n\n{file_content_string}\n\nWrite a git diff within ```diff and ``` that fully fixes the problem.\nThe git diff should not cause other tests to fail.\n\nExample:\n\n```diff\n--- a/first.txt\n+++ b/first.txt\n@@ -1,3 +1,3 @@\n start\n-first change\n+new first change\n middle\n@@ -7,4 +7,4 @@\n some content\n-second change\n+new second change\n more content\n--- a/second.txt\n+++ b/second.txt\n@@ -1,3 +1,3 @@\n beginning\n-old line\n+new line\n end\n```\n\nReminder\n- Put your diff within ```diff and ``` and make sure the diff is valid.\n- Only the last diff printed will be considered.\n\"\"\".strip()\n)\n\nimport re\n\n\ndef get_patch_string(\n    problem_statement: str, file_content_strings: List[str], num_candidates: int = 3\n) -> Tuple[List[str], List[Optional[str]]]:\n    \"\"\"\n    Generates patch candidates using diverse sampling by varying the sampling temperature.\n    Returns:\n      - A list of combined completion texts (one per file)\n      - A list of patch strings (one per file), selected from the candidate pool.\n    \"\"\"\n    # Identify indices for non-empty file content strings.\n    inference_idx_to_input_idx: List[int] = [\n        idx for idx, fc in enumerate(file_content_strings) if fc != \"\"\n    ]\n    \n    # Prepare the messages for each file.\n    list_of_messages: List[List[Dict[str, str]]] = [\n        [\n            {\n                \"role\": \"user\",\n                \"content\": patching_prompt.format(\n                    problem_statement=problem_statement[:20_000],\n                    file_content_string=file_content_strings[idx][:30_000],\n                ),\n            },\n        ]\n        for idx in inference_idx_to_input_idx\n    ]\n    \n    # Initialize dictionaries to store candidates for each file.\n    candidate_completion_texts = {idx: [] for idx in inference_idx_to_input_idx}\n    candidate_patch_strings = {idx: [] for idx in inference_idx_to_input_idx}\n    \n    # Loop to generate multiple candidates per input.\n    for candidate in range(num_candidates):\n        # Vary the temperature slightly for diversity.\n        candidate_sampling_params = SamplingParams(\n            temperature=0.6 + candidate * 0.1,\n            min_p=0.01,\n            skip_special_tokens=True,\n            max_tokens=MAX_TOKENS,\n        )\n        \n        # Build prompt texts for all messages.\n        prompt_texts: List[str] = [\n            tokenizer.apply_chat_template(\n                conversation=messages, tokenize=False, add_generation_prompt=True\n            ) + \"<think>\\n\"\n            for messages in list_of_messages\n        ]\n        \n        print(\"get_patch_string (candidate {}):\".format(candidate), [count_tokens(text) for text in prompt_texts])\n        \n        # Generate responses for this candidate round.\n        request_outputs: List[RequestOutput] = llm.generate(\n            prompt_texts, sampling_params=candidate_sampling_params\n        )\n        response_texts_from_inference: List[str] = [\n            request_output.outputs[0].text for request_output in request_outputs\n        ]\n        print(\"Candidate {} responses:\".format(candidate), [count_tokens(text) for text in response_texts_from_inference])\n        \n        # Combine prompt and response, then extract patch string.\n        completion_texts_from_inference = [\n            prompt_text + response_text\n            for prompt_text, response_text in zip(prompt_texts, response_texts_from_inference)\n        ]\n        patch_strings_from_inference: List[Optional[str]] = [\n            extract_patch_string(response_text)\n            for response_text in response_texts_from_inference\n        ]\n        \n        # Save the candidate outputs per file.\n        for idx, (comp_text, patch_str) in zip(inference_idx_to_input_idx, \n                                                 zip(completion_texts_from_inference, patch_strings_from_inference)):\n            candidate_completion_texts[idx].append(comp_text)\n            candidate_patch_strings[idx].append(patch_str)\n    \n    # For each file, select the first candidate with a valid (non-None) patch.\n    final_completion_texts: List[str] = [\"\" for _ in file_content_strings]\n    final_patch_strings: List[Optional[str]] = [None for _ in file_content_strings]\n    \n    for idx in inference_idx_to_input_idx:\n        selected_patch = None\n        selected_completion = \"\"\n        for comp_text, patch_str in zip(candidate_completion_texts[idx], candidate_patch_strings[idx]):\n            if patch_str is not None:\n                selected_patch = patch_str\n                selected_completion = comp_text\n                break\n        final_patch_strings[idx] = selected_patch\n        final_completion_texts[idx] = selected_completion\n\n    return final_completion_texts, final_patch_strings\n","metadata":{"_uuid":"3229d17a-0142-4003-8b3c-ba42ae72d4d9","_cell_guid":"2000b5e2-13f3-4b86-8743-4aff8a950f2c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-02-22T10:34:03.972206Z","iopub.execute_input":"2025-02-22T10:34:03.972402Z","iopub.status.idle":"2025-02-22T10:34:03.986389Z","shell.execute_reply.started":"2025-02-22T10:34:03.972385Z","shell.execute_reply":"2025-02-22T10:34:03.985808Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"from pathlib import Path\n\nverifying_prompt: str = (\n    \"\"\"\nThis is the problem statement.\n\n{problem_statement}\n\nThese are the files that is thought to be relevant, which may not be complete.\n\n{file_content_string}\n\nThis is the proposed patch to fix the problem.\n\n{patch_string}\n\nEvaluate whether the patch works\n- The patch fully fixes the problem described in the problem statement.\n- The patch does not cause side effects and make any other tests fail.\n\nEnd your response with exactly either of\n- <label>Yes</label>, this fixes the problem.\n- <label>No</label>, this does not fix the problem.\n\nReminder\n- Only evaluate, do not provide suggestion on how to fix.\n- Remember to write exactly either of <label>Yes</label> or <label>No</label> in the last line\n\"\"\".strip()\n)\n\n\ndef is_valid_patch_format(patch_string: str) -> bool:\n    \"\"\"\n    A quick check to confirm if a patch could be valid.\n    \"\"\"\n    if not(isinstance(patch_string, str)):\n        return False\n    try:\n        patch_set = unidiff.PatchSet(patch_string)\n        if len(patch_set) == 0:\n            return False\n    except Exception:\n        return False\n    return True\n\n\ndef patch_dry_run_succeeds(patch_string: str, repo_path: str = REPO_PATH, timeout: int = 60) -> bool:\n    \"\"\"\n    A robust check if the patch will proceed without any errors.\n    Should be run after `is_valid_patch_format()`: the patch\n    command can hang if the inputs are sufficiently invalid.\n\n    Args:\n        patch_path: Path to a file containing the patch.\n        repo_path: Path to the directory to be patched.\n        timeout: Number of seconds before the dry run will be cancelled.\n    \"\"\"\n    with open(\"patch.txt\", \"w\") as f:\n        f.write(patch_string)\n    patch_path = \"/kaggle/working/patch.txt\"\n\n    cmd = f\"patch --quiet --dry-run -p1 -i {patch_path} -d {repo_path}\"\n    try:\n        subprocess.run(cmd, shell=True, check=True, timeout=timeout)\n        return True\n    except subprocess.CalledProcessError:\n        return False\n\n\ndef get_verification(\n    problem_statement: str,\n    file_content_strings: List[str],\n    patch_strings: List[Optional[str]],\n    repo_path: str,\n) -> Tuple[List[List[str]], List[List[bool]]]:\n    assert len(file_content_strings) == len(patch_strings)\n    sampling_params: SamplingParams = SamplingParams(\n        temperature=0.6,  # randomness of the sampling\n        min_p=0.01,\n        skip_special_tokens=True,  # Whether to skip special tokens in the output\n        max_tokens=MAX_TOKENS,\n    )\n\n    inference_idx_to_input_idx: list[int] = [\n        input_idx\n        for _ in range(VALIDATION_COPY_COUNT)\n        for input_idx, patch_string in enumerate(patch_strings)\n        if patch_string is not None and is_valid_patch_format(patch_string) # and patch_dry_run_succeeds(patch_string, repo_path)\n    ]\n    print(inference_idx_to_input_idx)\n\n    list_of_messages: List[List[Dict[str, str]]] = [\n        [\n            {\n                \"role\": \"user\",\n                \"content\": verifying_prompt.format(\n                    problem_statement=problem_statement[:20_000],\n                    file_content_string=file_content_strings[input_idx][:30_000],\n                    patch_string=patch_strings[input_idx],\n                ),\n            },\n        ]\n        for input_idx in inference_idx_to_input_idx\n    ]\n\n    prompt_texts: List[str] = [\n        (\n            tokenizer.apply_chat_template(\n                conversation=messages, tokenize=False, add_generation_prompt=True\n            )  # type: ignore\n        )\n        + \"<think>\\n\"\n        for messages in list_of_messages\n    ]\n    # print(prompt_texts)\n\n    print(\"get_verification\", [count_tokens(text) for text in prompt_texts])\n    request_outputs: list[RequestOutput] = llm.generate(\n        prompt_texts, sampling_params=sampling_params\n    )\n    response_texts: List[str] = [\n        request_output.outputs[0].text for request_output in request_outputs\n    ]\n    print(\"get_verification\", [count_tokens(text) for text in response_texts])\n\n    completion_texts = [\n        prompt_text + response_text\n        for prompt_text, response_text in zip(prompt_texts, response_texts)\n    ]\n    judgments_flattened: List[bool] = [\n        \"<label>Yes</label>\" in response_text for response_text in response_texts\n    ]\n    print(judgments_flattened)\n\n    judgments_aggregated: List[List[bool]] = [[] for _ in file_content_strings]\n    completion_text_aggregated: List[List[str]] = [[] for _ in patch_strings]\n    for inference_idx, (completion_text, judgement) in enumerate(\n        zip(completion_texts, judgments_flattened)\n    ):\n        input_idx = inference_idx_to_input_idx[inference_idx]\n        completion_text_aggregated[input_idx].append(completion_text)\n        judgments_aggregated[input_idx].append(judgement)\n    print(judgments_aggregated)\n\n    return completion_text_aggregated, judgments_aggregated","metadata":{"_uuid":"5f229582-95fc-48d2-ac8e-27e71fd5e4c9","_cell_guid":"b7e7aec9-62b4-4dc2-af35-b8f01bdcff73","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-02-22T10:34:03.987039Z","iopub.execute_input":"2025-02-22T10:34:03.987250Z","iopub.status.idle":"2025-02-22T10:34:04.000666Z","shell.execute_reply.started":"2025-02-22T10:34:03.987224Z","shell.execute_reply":"2025-02-22T10:34:04.000115Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"import unidiff\nimport subprocess\n\n\ndef choose_patch_string(\n    patch_strings: list[Optional[str]], judgments_aggregated: List[List[bool]], repo_path: str\n) -> tuple[list[int], Optional[str]]:\n    best_score = -4\n    best_patch_string = None\n\n    scores = []\n    for judgments, patch_string in zip(judgments_aggregated, patch_strings):\n\n        if patch_string is None:\n            score = -3\n            scores.append(score)\n            continue\n        \n        if not is_valid_patch_format(patch_string):\n            score = -2\n            scores.append(score)\n            continue\n        \n        if not patch_dry_run_succeeds(patch_string, repo_path):\n            score = -1\n            scores.append(score)\n            continue\n        \n        score = judgments.count(True)\n        scores.append(score)\n        \n        if score > best_score:\n            best_score = score\n            best_patch_string = patch_string\n\n    return scores, best_patch_string","metadata":{"_uuid":"ac627a45-b48c-4bc3-b3e8-ff0ee18b3fc6","_cell_guid":"9814b304-d64c-4d61-b1d7-a4ab7b85496f","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-22T10:34:30.680729Z","iopub.execute_input":"2025-02-22T10:34:30.681066Z","iopub.status.idle":"2025-02-22T10:34:30.686418Z","shell.execute_reply.started":"2025-02-22T10:34:30.681042Z","shell.execute_reply":"2025-02-22T10:34:30.685798Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"# Predict function","metadata":{"_uuid":"332ef3ee-d385-4103-8c68-d547dda443bf","_cell_guid":"d0812f8b-c4aa-47f0-a3be-36d135908ffc","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"def predict_inner(problem_statement: str, directory: str) -> Optional[str]:\n    directory_string = stringify_directory(directory)\n    \n    # Run file selection once\n    selection_completion_texts, file_queries = get_selection_query(\n        directory_string, problem_statement\n    )\n    file_content_strings: List[str] = [\n        fetch_file_contents(file_query) for file_query in file_queries\n    ]\n    \n    max_iterations = 3  # Maximum number of refinement iterations\n    best_patch = None\n    best_score = -float(\"inf\")\n    final_iteration_data = None  # Will hold data from the best iteration\n    \n    # Iterative refinement loop\n    for iteration in range(max_iterations):\n        print(f\"Iteration {iteration + 1} of {max_iterations}\")\n        # Generate patch candidates (you might also adjust sampling parameters here)\n        patch_completion_texts, patch_strings = get_patch_string(\n            problem_statement, file_content_strings\n        )\n        # Verify the generated patches\n        verification_completion_texts_aggregated, judgments_aggregated = get_verification(\n            problem_statement, file_content_strings, patch_strings, directory\n        )\n        # Choose the best patch from the current iteration\n        scores, patch_candidate = choose_patch_string(patch_strings, judgments_aggregated, directory)\n        \n        # For simplicity, we use the maximum score from this iteration (could also use an average)\n        iteration_best = max(scores) if scores else -float(\"inf\")\n        print(f\"Iteration {iteration + 1} best score: {iteration_best}\")\n        \n        # Update our best candidate if this iteration is better\n        if iteration_best > best_score:\n            best_score = iteration_best\n            best_patch = patch_candidate\n            final_iteration_data = {\n                \"patch_completion_text\": patch_completion_texts,\n                \"patch_strings\": patch_strings,\n                \"scores\": scores,\n                \"judgments_aggregated\": judgments_aggregated,\n            }\n        \n        # Break early if a satisfactory patch is found (e.g., at least one positive judgment)\n        if best_score >= 1:\n            print(\"Satisfactory patch found; breaking early.\")\n            break\n\n    # Logging the iteration data if not in a competition rerun.\n    if not os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\") and final_iteration_data is not None:\n        data = {\n            \"problem_statement\": [problem_statement] * len(file_queries),\n            \"selection_completion_text\": selection_completion_texts,\n            \"selection_completion_length\": [\n                count_tokens(text) for text in selection_completion_texts\n            ],\n            \"file_query\": file_queries,\n            \"file_content_string\": file_content_strings,\n            \"patch_completion_text\": final_iteration_data[\"patch_completion_text\"],\n            \"patch_completion_length\": [\n                count_tokens(text) for text in final_iteration_data[\"patch_completion_text\"]\n            ],\n            \"patch_string\": final_iteration_data[\"patch_strings\"],\n            \"score\": final_iteration_data[\"scores\"],\n            \"best_score\": best_score,\n        }\n        # Optionally log verification details for each copy.\n        for copy_idx in range(VALIDATION_COPY_COUNT):\n            data[f\"verification_completion_text_{copy_idx}\"] = [\n                completion_texts[copy_idx] if completion_texts else None\n                for completion_texts in verification_completion_texts_aggregated\n            ]\n            data[f\"verification_completion_length_{copy_idx}\"] = [\n                count_tokens(completion_texts[copy_idx]) if completion_texts else None\n                for completion_texts in verification_completion_texts_aggregated\n            ]\n            data[f\"judgment_{copy_idx}\"] = [\n                judgments[copy_idx] if judgments else None\n                for judgments in judgments_aggregated\n            ]\n        data[\"judgment_count_true\"] = [judgments.count(True) for judgments in judgments_aggregated]\n        pd.DataFrame(data).to_csv(\n            f\"{str(int(time.time() - start_time)).zfill(5)}.csv\", index=False\n        )\n    \n    return best_patch\n","metadata":{"_uuid":"d5aa7692-14fa-46cb-81aa-4a05d2990df4","_cell_guid":"41d164a7-440a-4b3d-b82b-9473575c89a4","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-02-22T10:35:04.030223Z","iopub.execute_input":"2025-02-22T10:35:04.030563Z","iopub.status.idle":"2025-02-22T10:35:04.039152Z","shell.execute_reply.started":"2025-02-22T10:35:04.030538Z","shell.execute_reply":"2025-02-22T10:35:04.038441Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"import io\nfrom typing import Optional, List\n\nskip_prediction: bool = False\n\n\ndef predict(\n    problem_statement: str,\n    repo_archive: io.BytesIO,\n    pip_packages_archive: io.BytesIO,\n    env_setup_cmds_templates: List[str],\n) -> Optional[str]:\n    \"\"\"Replace this function with your inference code.\n    Args:\n        problem_statement: The text of the git issue.\n        repo_archive: A BytesIO buffer path with a .tar containing the codebase that must be patched. The gateway will make this directory available immediately before this function runs.\n    \"\"\"\n    global skip_prediction\n    if skip_prediction:\n        return None\n\n    with open(\"repo_archive.tar\", \"wb\") as f:\n        f.write(repo_archive.read())\n    repo_path: str = REPO_PATH\n    if os.path.exists(repo_path):\n        shutil.rmtree(repo_path)\n    shutil.unpack_archive(\"repo_archive.tar\", extract_dir=repo_path)\n    os.remove(\"repo_archive.tar\")\n\n    patch_string: Optional[str] = None\n    patch_string = predict_inner(\n        problem_statement=problem_statement, directory=repo_path\n    )\n    shutil.rmtree(repo_path)\n\n    if not os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n        skip_prediction = True\n\n    print(\"submitted patch_string\")\n    print(patch_string)\n\n    if patch_string is None:\n        return None\n\n    return patch_string","metadata":{"_uuid":"1fe33c0a-f4bb-4d58-a31a-8a35efed47f4","_cell_guid":"f6603ce1-36ca-4c32-b6b7-55d970a38851","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-22T10:35:04.713289Z","iopub.execute_input":"2025-02-22T10:35:04.713594Z","iopub.status.idle":"2025-02-22T10:35:04.719156Z","shell.execute_reply.started":"2025-02-22T10:35:04.713570Z","shell.execute_reply":"2025-02-22T10:35:04.718447Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"# Get predict data without server","metadata":{"_uuid":"ec5157f3-fc1a-4944-a94b-d472995e12cd","_cell_guid":"81b9a808-f41a-4715-aafc-73184d4cc947","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import os\nimport zipfile\n\n# !mkdir -p /kaggle/tmp/konwinski-prize-alt\nos.makedirs(\"/kaggle/tmp/konwinski-prize-alt\", exist_ok=True)\n\n# !unzip -q -o /kaggle/input/konwinski-prize/data.a_zip -d /kaggle/tmp/konwinski-prize-alt/ 2>/dev/null || true\ntry:\n    with zipfile.ZipFile(\"/kaggle/input/konwinski-prize/data.a_zip\", \"r\") as zip_ref:\n        zip_ref.extractall(\"/kaggle/tmp/konwinski-prize-alt/\")\nexcept:\n    pass","metadata":{"_uuid":"6d66b2c6-91e0-4652-b671-a2534be7e103","_cell_guid":"91157abf-a02f-47f2-b6b2-64057f65adaa","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-22T10:35:08.009041Z","iopub.execute_input":"2025-02-22T10:35:08.009340Z","iopub.status.idle":"2025-02-22T10:35:12.364496Z","shell.execute_reply.started":"2025-02-22T10:35:08.009316Z","shell.execute_reply":"2025-02-22T10:35:12.363767Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"import pandas as pd\n\n\ndef get_problem(problem_index: int) -> Tuple[str, str, io.BytesIO]:\n    df = pd.read_parquet(\"/kaggle/tmp/konwinski-prize-alt/data/data.parquet\")\n\n    problem_statement: str = df[\"problem_statement\"][problem_index]\n    repo_path: str = (\n        f\"/kaggle/tmp/konwinski-prize-alt/data/repos/repo__{df['instance_id'][problem_index]}\"\n    )\n\n    import shutil\n    import tempfile\n\n    with tempfile.TemporaryDirectory() as tmpdir:\n        shutil.make_archive(os.path.join(tmpdir, \"a_repo\"), \"tar\", repo_path)\n        with open(os.path.join(tmpdir, \"a_repo.tar\"), \"rb\") as f:\n            repo_archive = io.BytesIO(f.read())\n\n    return problem_statement, repo_path, repo_archive","metadata":{"_uuid":"46af49a7-33eb-4d09-9830-7c96f125cae4","_cell_guid":"eb711b99-4aff-48a8-8512-a00e18923f41","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-22T10:35:26.378445Z","iopub.execute_input":"2025-02-22T10:35:26.378782Z","iopub.status.idle":"2025-02-22T10:35:26.383447Z","shell.execute_reply.started":"2025-02-22T10:35:26.378731Z","shell.execute_reply":"2025-02-22T10:35:26.382813Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"demo_problem_index: int = 0\n\nif os.getenv(\"KAGGLE_KERNEL_RUN_TYPE\") == \"Interactive\" and not os.getenv(\n    \"KAGGLE_IS_COMPETITION_RERUN\"\n):\n    problem_statement, repo_path, repo_archive = get_problem(\n        problem_index=demo_problem_index\n    )\n\n    print(repo_path)\n    print(problem_statement)\n    print(len(list(repo_archive)))\n    print(len(list(repo_archive)))","metadata":{"_uuid":"71445695-cf75-4bdf-bf38-e9d5fdc7c07c","_cell_guid":"61fe4186-964b-4385-8837-e2f33c44c722","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-22T10:35:27.658070Z","iopub.execute_input":"2025-02-22T10:35:27.658369Z","iopub.status.idle":"2025-02-22T10:35:27.795813Z","shell.execute_reply.started":"2025-02-22T10:35:27.658346Z","shell.execute_reply":"2025-02-22T10:35:27.795119Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"/kaggle/tmp/konwinski-prize-alt/data/repos/repo__pylint-dev__astroid-2496\nTypeError: unsupported format string passed to NoneType.__format__\nRegression in #2459\n\n### Steps to reproduce\na.py:\n```py\nclass A:\n    def __init__(self):\n        self._magnitude = None\n\n    def name(self) -> str | None:\n        if self._magnitude:\n            return f\"M {self._magnitude:.1f}\"\n```\n```\npylint a.py\n```\n### Current behavior\n```\n  File \"/Users/jwalls/release/lib/python3.12/site-packages/astroid/nodes/node_classes.py\", line 4778, in _infer_from_values\n    yield from nodes[0]._infer(context, **kwargs)\n  File \"/Users/jwalls/release/lib/python3.12/site-packages/astroid/nodes/node_classes.py\", line 4695, in _infer\n    formatted = format(value.value, format_spec.value)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: unsupported format string passed to NoneType.__format__\n```\n\n68654\n0\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"if os.getenv(\"KAGGLE_KERNEL_RUN_TYPE\") == \"Interactive\" and not os.getenv(\n    \"KAGGLE_IS_COMPETITION_RERUN\"\n):\n    skip_prediction = False\n    problem_statement, repo_path, repo_archive = get_problem(\n        problem_index=demo_problem_index\n    )\n    patch_string = predict(problem_statement, repo_archive, io.BytesIO(), [])","metadata":{"_uuid":"40eac664-ddd7-462d-b9a2-6a65353e8b5a","_cell_guid":"ce7abf42-5abe-41b4-aca7-3c7302749713","trusted":true,"collapsed":false,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2025-02-22T10:35:32.416979Z","iopub.execute_input":"2025-02-22T10:35:32.417328Z","iopub.status.idle":"2025-02-22T10:42:38.543138Z","shell.execute_reply.started":"2025-02-22T10:35:32.417302Z","shell.execute_reply":"2025-02-22T10:42:38.542402Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"get_selection_query [4197, 4197, 4197, 4197, 4197, 4197]\n","output_type":"stream"},{"name":"stderr","text":"Processed prompts: 100%|██████████| 6/6 [01:06<00:00, 11.11s/it, est. speed input: 377.69 toks/s, output: 95.35 toks/s]\n","output_type":"stream"},{"name":"stdout","text":"get_selection_query [791, 1181, 1003, 1042, 1381, 959]\nIteration 1 of 3\nget_patch_string (candidate 0): [845, 875, 860, 853, 854, 896]\n","output_type":"stream"},{"name":"stderr","text":"Processed prompts: 100%|██████████| 6/6 [01:54<00:00, 19.08s/it, est. speed input: 45.28 toks/s, output: 137.73 toks/s]\n","output_type":"stream"},{"name":"stdout","text":"Candidate 0 responses: [2835, 2175, 3494, 2103, 3438, 1720]\nget_patch_string (candidate 1): [845, 875, 860, 853, 854, 896]\n","output_type":"stream"},{"name":"stderr","text":"Processed prompts: 100%|██████████| 6/6 [01:39<00:00, 16.57s/it, est. speed input: 52.13 toks/s, output: 122.88 toks/s]\n","output_type":"stream"},{"name":"stdout","text":"Candidate 1 responses: [3140, 2905, 1441, 1943, 1782, 1007]\nget_patch_string (candidate 2): [845, 875, 860, 853, 854, 896]\n","output_type":"stream"},{"name":"stderr","text":"Processed prompts: 100%|██████████| 6/6 [02:09<00:00, 21.54s/it, est. speed input: 40.11 toks/s, output: 114.28 toks/s]\n","output_type":"stream"},{"name":"stdout","text":"Candidate 2 responses: [2463, 2180, 342, 2306, 4097, 3381]\n[0, 3, 5]\nget_verification [895, 925, 987]\n","output_type":"stream"},{"name":"stderr","text":"Processed prompts: 100%|██████████| 3/3 [00:15<00:00,  5.19s/it, est. speed input: 180.47 toks/s, output: 86.28 toks/s]\n","output_type":"stream"},{"name":"stdout","text":"get_verification [392, 488, 461]\n[True, True, True]\n[[True], [], [], [True], [], [True]]\nIteration 1 best score: 1\nSatisfactory patch found; breaking early.\nsubmitted patch_string\n--- a/astroid/nodes/node_classes.py\n+++ b/astroid/nodes/node_classes.py\n@@ -4692,6 +4692,9 @@\n                         yield util.Uninferable\n                         uninferable_already_generated = True\n                     continue\n+                if value.value is None:\n+                    yield util.Uninferable\n+                    continue\n                 formatted = format(value.value, format_spec.value)\n                 yield Const(\n                     formatted,\n\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"if (\n    os.getenv(\"KAGGLE_KERNEL_RUN_TYPE\") == \"Interactive\"\n    and not os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\")\n    and patch_string is not None\n):\n    import polars as pl\n\n    df = pl.read_parquet(\"/kaggle/tmp/konwinski-prize-alt/data/data.parquet\")\n\n    import kaggle_evaluation.konwinski_prize_gateway\n\n    k_prize_gateway = kaggle_evaluation.konwinski_prize_gateway.KPrizeGateway()\n    k_prize_gateway.unpack_data_paths()\n\n    results = k_prize_gateway._evaluate_instance(\n        instance=df.row(demo_problem_index, named=True),\n        patch=patch_string,\n    )\n\n    from collections import Counter\n    print(\n        demo_problem_index, Counter(result.unit_test_outcome for result in results[1:])\n    )","metadata":{"_uuid":"c00544c0-d176-4d48-a025-6cbac48576ec","_cell_guid":"11968fc9-c39e-4088-b953-74ea01632ad3","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-22T10:42:38.544092Z","iopub.execute_input":"2025-02-22T10:42:38.544322Z","iopub.status.idle":"2025-02-22T10:42:56.428393Z","shell.execute_reply.started":"2025-02-22T10:42:38.544303Z","shell.execute_reply":"2025-02-22T10:42:56.427620Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"Existing uv installation found. Skipping uv installation.\nInstalling Python 3.11...\n0 Counter({<UnitTestOutcome.PASSED: 'passed'>: 426, <UnitTestOutcome.FAILED: 'failed'>: 1})\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"if (\n    os.getenv(\"KAGGLE_KERNEL_RUN_TYPE\") == \"Interactive\"\n    and not os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\")\n    and patch_string is not None\n):\n    from kaggle_evaluation.konwinski_prize_gateway import UnitTestOutcome\n\n    for result in results[1:]:\n        if result.unit_test_outcome != UnitTestOutcome.PASSED:\n            print(result.test_name)\n            print(result.fail_description)\n            ","metadata":{"_uuid":"db5f7042-adf4-42c0-918e-cdb9031cd545","_cell_guid":"74a18506-0c71-4c43-828e-ac63c9f066a0","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-22T10:42:56.429553Z","iopub.execute_input":"2025-02-22T10:42:56.429846Z","iopub.status.idle":"2025-02-22T10:42:56.434197Z","shell.execute_reply.started":"2025-02-22T10:42:56.429808Z","shell.execute_reply":"2025-02-22T10:42:56.433523Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"tests/test_inference.py::test_formatted_fstring_inference[width = None\\nprecision = 4\\nvalue = 12.34567\\nresult = f\"result: {value:{width}.{precision}}!\"\\n-None]\nNone\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"When your notebook is run on the hidden test set, inference_server.serve must be called within 15 minutes of the notebook starting or the gateway will throw an error. If you need more than 15 minutes to load your model you can do so during the very first predict call, which does not have the usual 30 minute response deadline.","metadata":{"_uuid":"3547dec1-18d2-49b0-8c08-ac09d8193723","_cell_guid":"3c0427bb-5570-4314-83bd-bc17d09fe3c7","trusted":true,"collapsed":false,"papermill":{"duration":0.001889,"end_time":"2024-12-11T03:22:08.856283","exception":false,"start_time":"2024-12-11T03:22:08.854394","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"# Evaluation with inference server","metadata":{"_uuid":"c6870d06-100c-4160-9cf5-311910a84007","_cell_guid":"ac0ac0e2-d8c1-4e8b-8415-ae41b202d87d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"skip_prediction = False","metadata":{"_uuid":"6a96bb2f-85cc-4b37-b044-1ecbf8422efd","_cell_guid":"697c6852-96c5-4c3f-85af-64b340b1c65c","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-22T10:43:29.498020Z","iopub.execute_input":"2025-02-22T10:43:29.498330Z","iopub.status.idle":"2025-02-22T10:43:29.501875Z","shell.execute_reply.started":"2025-02-22T10:43:29.498307Z","shell.execute_reply":"2025-02-22T10:43:29.501173Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"inference_server = (\n    kaggle_evaluation.konwinski_prize_inference_server.KPrizeInferenceServer(\n        get_number_of_instances, predict\n    )\n)\n\nif os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n    inference_server.serve()\nelse:\n    inference_server.run_local_gateway(\n        data_paths=(\n            \"/kaggle/input/konwinski-prize/\",  # Path to the entire competition dataset\n            \"/kaggle/tmp/konwinski-prize/\",  # Path to a scratch directory for unpacking data.a_zip.\n        ),  # type: ignore\n        \n        use_concurrency=True,\n    )","metadata":{"_uuid":"e03ade0d-f48f-4682-888e-bd8d258767f3","_cell_guid":"bbbf2217-ac29-4d86-bffd-afe648ae1167","trusted":true,"collapsed":false,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2025-02-22T10:43:29.860906Z","iopub.execute_input":"2025-02-22T10:43:29.861199Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"Existing uv installation found. Skipping uv installation.\nget_selection_query [4197, 4197, 4197, 4197, 4197, 4197]\n","output_type":"stream"},{"name":"stderr","text":"Processed prompts:  83%|████████▎ | 5/6 [01:01<00:07,  7.69s/it, est. speed input: 340.84 toks/s, output: 78.34 toks/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"_uuid":"86683ecc-947a-444b-b073-352bf009e91c","_cell_guid":"b73f8a85-f747-4a0d-9598-6bf418752bc6","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}